[{"content":"集成学习 知识补充 弱学习器 在给定任务中，其性能略好于随机猜测的学习模型，但是显著弱于强学习器。\n强学习器 在给定任务中，性能远超随机猜测的学习模型。它能够有效地从数据中学习并做出准确的预测，通常具有较高的准确率和较好的泛化能力。\n串行 下一个分类器只在前一个分类器预测不够准确的实例上进行训练或者检验。\n并行 所有的弱分类器都给出各自的预测结果，通过组合把这些预测结果转化为最终结果。\n定义 集成学习是使用一系列学习器进行学习，并使用某种规则把各个学习结果进行整合从而获得比单个学习器更好的学习效果的一种机器学习方法。\n集成学习通过将多个学习器进行结合，可获得比单一学习器显著卓越的泛化性能，对弱学习器最明显，因此集成学习的很多理论学习都是针对弱学习器进行的。\n相关问题 问题1：弱学习器之间的关系 第一种就是所有个体学习器都是一个种类的，也可以称为同质的。例如决策树的集成全是决策树，神经网络集成中全是神经网络。\n第二种就是所有个体学习器不全是一个种类的，也可以称为异质的。例如同时包含决策树和神经网络。\n其中同质个体学习器按照个体学习器之间的依赖关系又分两类。一类是当一系列个体学习器存在强依赖关系，都要串行生成，代表为\\(Boosting\\)系列算法，第二类是个体学习器之间不存在强依赖关系，一系列个体学习器可以并行生成，代表算法是\\(bagging\\)和随机森林。\n问题2：学习器的选择 我们在选择学习器时要秉持好而不同的原则。既要考虑准确性，也要考虑多样性。\n准确性：是指个体学习器不能太差，要有一定的准确度。\n多样性：是指个体学习器之间的输出要有差异性。\n简单举例，下图是一个二分类任务的结果，对号表示分类正确。集成结果少数服从多数。 问题3：怎样组合弱学习器 假定集成包含T个基学习器\\(\\{ h_1,h_2,……h_T \\}\\)，其中\\(h_i\\)在示例\\(x\\)上的输出为\\(h_i(x)\\)。\n平均法 对数值型输出，数值类的回归预测问题比较常用。\n简单平均法 \\[ H(x) = \\frac{1}{T} \\sum_{i=1}^{T} h_i(x) \\]加权平均法 \\[ H(x) = \\sum_{i=1}^{T} w_i \\cdot h_i(x) \\] 其中 \\( w_i \\) 是第 \\( i \\) 个个体学习器的权重，满足 \\( \\sum_{i=1}^{T} w_i = 1，w_i \\geq 0 \\)。\n投票法 对于分类问题比较常见。\n常见的投票方法有绝对多数投票法(若某标记记得票数超过半数，则预测为该标记)，相对多数投票法(预测为得票最多的标记，若有多个标记获得最高票，则从中随机选取)，加权投票法等。\n学习法 当训练数据很多时，我们可以采取学习法，即通过另一个学习器来进行结合，\\(Stacking\\)是学习法的典型代表，我们将个体学习器称为初级学习器，用于结合的学习器称为元学习器。\n步骤如下\n\\(1\\).将训练数据集划分为多个子集，通常是两个或更多个。\n\\(2\\).对于每个子集，使用不同的初级学习器进行训练和预测，得到每个初级学习器的预测结果。\n\\(3\\).将这些预测结果作为新的特征，组合成一个新的训练数据集。\n\\(4\\).使用这个新的训练数据集来训练一个元学习器，例如逻辑回归、决策树等。\n\\(5\\).最后，使用训练好的元学习器来对测试数据进行预测。\nBoosting 算法介绍 Boosting是可以将弱学习器提升为强学习器的算法：先从初始训练集训练出一个基学习器，再根据基学习器的表现对训练样本分布进行调整，使得先前基学习器做错的训练样本在后续收到更多关注，然后基于调整后的样本分布来训练下一个基学习器，如此反复进行，直至 基学习器数目达到事先指定的值T，最终将这T个基学习器进行加权结合。\n即采用了重赋权法：即在训练过程的每一轮中，根据样本分布为每一个训练样本重新赋予一个权重。对无法接受带权样本的 基学习算法，则可以通过重采样法来处理，即在每一轮的学习中，根据样本分布对训练集重新进行采样，在对重采样而来的样本集对基学习器进行训练。 算法补充 1.准确率越高的弱学习机权重越高。\nAdaboost \\(Adaboost\\)全称为\\((adaptive \\ boosting)\\),\\(adaptive\\)的意思是自适应，那么与\\(boosting\\)相比，\\(Adaboost\\)是如何去实现自适应的呢？\\(Adaboost\\)需要解决两个问题。\n1.在每一轮训练中，如何对训练样本的权值分布进行调整？\n2.如何确定各个基学习器“加权”结合的权重？\nAdaBoost的解决思路 通过分类器权重公式来确定各个基学习器的权重；通过样本分布更新公式来确定各轮训练中的样本分布。\n1.在确定各个基学习器的权重时：\n加大“分类误差率”小的弱分类器的权值，使其在表决中发挥较大作用；减小“分类误差率”大的弱分类器的权值，使其在表决中发挥较小作用。\n2.在确定各轮训练中的样本分布时：\n提高那些被前一轮弱分类器错误分类样本的权值，降低那些被正确分类的样本的权值。\n算法流程 Adaboost公式推导 推导前置说明 \\(Adaboost\\)有多种推导方式，我们以基于加法模型来进行推导。 推导的目标是最小化目标损失函数。\n注意，我们在这里讨论的是二分类问题，其中训练样本集的 \\(y_i∈\\{−1,+1\\}\\)，\\(f\\) 是真实函数。对于样本\\(x\\)来说，真实值\\(f(x)\\)∈\\(\\{−1,+1\\}\\)，每个基学习器的预测值 \\(h_i(x)∈\\{−1,+1\\}\\)，假定基分类器的错误率为 \\(ϵ\\)，即对每个基分类器\\(h_i\\)有： $$ P(h_i(x)\\neq f(x))=\\epsilon \\tag{8.1} $$ 假设集成通过简单投票法结合 T个基分类器，若有超过半数的基分类器正确，则集成分类就正确；集成分类的结果 F简单投票(x) 为： $$ F_{简单投票}(x)=\\mathrm{sign}\\Big(\\sum\\limits_{i=1}^{T}h_i(x)\\Big) \\tag{8.2} $$ “加法模型”（\\(additive model\\)），即基学习器的线性组合 \\(H(x)\\)： $$ H(x)=\\sum\\limits_{t=1}^T\\alpha_{t}h_{t}(x) \\tag{8.4} $$ 其中，\\(\\alpha_t\\) 表示基分类器的权重，因为 AdaBoost 并不使用简单投票法，而是“加权表决法”，即让基学习器具有不同的权值： $$ F(x)=\\mathrm{sign}(H(x))=\\mathrm{sign}\\Big(\\sum\\limits_{t=1}^T\\alpha_{t}h_{t}(x)\\Big) $$ 指数损失函数”的定义为： $$ \\mathscr{l}_{exp}(H\\mid D)=\\mathbb{E}{x\\sim D}[e^{-f(x)H(x)}]\\tag{8.5} $$ 其中，\\(x\\sim D\\) 表示概率分布 D(x)，\\(\\mathbb{E}\\) 表示期望。\n这个指数损失函数为什么合理：当对于某一学习器而言，\\(Adaboost\\)的优点就是给予其权重，我们注意到\\(e\\)的指数为\\(-H(x)*f(x)\\)，当对于某一样本\\(x\\)，预测正确时 \\(H(x)\\)和\\(f(x)\\)应该为同号，前面加上符号，指数就变成了负数，同时若\\(H(x)\\)的值越大(分类器对结果的信任程度就高)，整个函数的值就越小。同时若分类错误，\\(H(x)\\)与\\(f(x)\\) 为异号，那么\\(e\\)的指数变为一个正数，且\\(H(x)\\)越大，整个损失函数的值都会变大(分类器错付了，那么损失函数值就要反应出问题)。\n在搞懂前提后，我们还需要借助两个公式。\n\\( \\mathbb{E}_{x\\sim D}[f(x)]=\\sum_{x\\in D}D(x)f(x)\\tag{工具公式1} \\) \\(\\sum\\limits_{i=1}^{|D|}D(x_i)\\mathbb{I}(f(x_i)=1)=p(f(x_i)=1|x_i)\\tag{工具公式2}\\)\n损失函数正确性说明 若集成学习器 \\(H(x)\\)能令指数损失函数最小化，则考虑式（8.5）对 \\(H(x)\\)的偏导： $$ \\dfrac{\\partial\\mathscr{l}_{exp}(H\\mid D)}{\\partial H(x)}=-e^{-H(x)}P(f(x)=1\\mid x)+e^{H(x)}P(f(x)=-1\\mid x)\\tag{8.6} $$ 令式(8.6)为0 将工具公式1代入式（8.5）： $$ \\mathscr{l}_{exp}(H\\mid D)=\\mathbb{E}{x\\sim D}[e^{-f(x)H(x)}]=\\sum\\limits_{x\\in D}D(x)e^{-f(x)H(x)} $$ 由于工具公式2： $$ \\sum\\limits_{i=1}^{|D|}D(x_i)\\mathbb{I}(f(x_i)=1)=P(f(x_i)=1\\mid x_i) $$ 又注意到 \\(f(x_i)∈−\\{1,+1\\}\\)，所以： $$ \\begin{aligned} \\dfrac{\\partial \\mathscr{l}_{exp}(H \\mid D)}{\\partial H(x)} \u0026= \\sum\\limits_{i=1}^{|D|} D(x_i) \\Big( -e^{-H(x_i)} \\mathbb{I}(f(x_i)=1) + e^{H(x_i)} \\mathbb{I}(f(x_i)=-1) \\Big) \\end{aligned}\n$$ \\begin{aligned} \u0026amp;=-e^{-H(x_i)} P(f(x_i)=1 \\mid x_i) + e^{H(x_i)} P(f(x_i)=-1 \\mid x_i) \\quad \\text{(由工具公式2得)} \\end{aligned} 式（8.6）的具体推导过程至此结束。 令式（8.6）为零，进行求解： $$ -e^{-H(x)} P(f(x)=1 \\mid x) + e^{H(x)} P(f(x)=-1 \\mid x) = 0 $$\\[ H(x) + \\ln(P(f(x)=-1 \\mid x)) = -H(x) + \\ln(P(f(x)=1 \\mid x) \\] 可解得：\n$$ H(x) = \\dfrac{1}{2} \\ln \\dfrac{P(f(x)=1 \\mid x)}{P(f(x)=-1 \\mid x)} \\tag{8.7} $$因此，有：\n$$ \\begin{aligned} \\mathrm{sign}(H(x)) \u0026= \\mathrm{sign}\\Big(\\dfrac{1}{2} \\ln \\dfrac{P(f(x)=1 \\mid x)}{P(f(x)=-1 \\mid x)}\\Big) \\\\ \u0026= \\begin{cases} 1, \u0026 P(f(x)=1 \\mid x) \u003e P(f(x)=-1 \\mid x) \\\\ -1, \u0026 P(f(x)=1 \\mid x) \u003c P(f(x)=-1 \\mid x) \\end{cases} \\\\ \u0026= \\mathop{\\mathrm{argmax}}\\limits_{y \\in \\{-1, 1\\}} P(f(x)=y \\mid x) \\end{aligned} \\tag{8.8} $$ 这意味着 \\(\\mathrm{sign}(H(x))\\) 达到了贝叶斯最优错误率（即对于每个样本\\(x\\) 都选择后验概率最大的类别）。换言之，若指数损失函数最小化，则分类错误率也将最小化；这说明指数损失函数是分类任务原本 0/1 损失函数的一致的\\(（consistent）\\)替代损失函数。由于这个替代损失函数有更好的数学性质，例如它是连续可微函数，因此我们用它替代 0/1 损失函数作为优化目标。 至此，我们知道了使用“指数损失函数”的原因及其正确性。\n分类器权重公式 下面，我们将从指数损失函数入手，推导\\(AdaBoost\\)算法的分类器权重公式。在\\(AdaBoost\\)算法中，第一个基分类器\\(h_1\\)是通过直接将基学习器算法用于初始数据分布而得；此后迭代地生成\\(h_t\\)和\\(\\alpha_t\\)，当基分类器\\(h_t\\)基于分布\\(D_t\\)产生后，该基分类器的权重\\(\\alpha_t\\)应使得 \\(\\alpha_th_t\\)最小化指数损失函数。 此时的指数损失函数： 由定义式(8.5)得到 $$ \\begin{aligned} \\mathscr{l}_{exp}(\\alpha_t h_t \\mid D_t) \u0026= \\mathbb{E}_{x \\sim D_t}[e^{-f(x) \\alpha_t h_t(x)}] \\end{aligned} $$ 分类讨论（两种情形）： $$ \\begin{aligned} \u0026=\\mathbb{E}_{x \\sim D_t}\\left[e^{-\\alpha_t} \\mathbb{I}(f(x) = h_t(x)) + e^{\\alpha_t} \\mathbb{I}(f(x) \\neq h_t(x))\\right] \\end{aligned} $$ 由工具公式1得到： $$ \\begin{aligned} \u0026=\\sum\\limits_{x \\in D_t} D_t(x) \\left[e^{-\\alpha_t} \\mathbb{I}(f(x) = h_t(x)) + e^{\\alpha_t} \\mathbb{I}(f(x) \\neq h_t(x))\\right] \\end{aligned} $$ 由工具公式2得到： $$ \\begin{aligned} \u0026= e^{-\\alpha_t} P_{x \\sim D_t}(f(x) = h_t(x)) + e^{\\alpha_t} P_{x \\sim D_t}(f(x) \\neq h_t(x)) \\end{aligned} $$$$ \\begin{aligned} \u0026= e^{-\\alpha_t}(1 - \\epsilon_t) + e^{\\alpha_t} \\epsilon_t \\quad \\text{错误率的定义推出} \\end{aligned} \\tag{8.9} $$ 其中，错误率 \\(\\epsilon_t = P_{x \\sim D_t}(h_t(x) \\neq f(x))\\)。 考虑指数损失函数的导数： $$ \\begin{aligned} \u0026\\dfrac{\\partial\\mathscr{l}_{exp}(\\alpha_th_t\\mid D_{t})}{\\partial\\alpha_t}=-e^{-\\alpha_t}(1-\\epsilon_t)+e^{\\alpha_t}\\epsilon_{t} \u0026由式(8.9)对\\alpha_{t}求偏导 \\end{aligned}\\tag{8.10} $$ 令式（8.10）为零，进行求解： $$ \\begin{aligned} \u0026-e^{-\\alpha_{t}}(1-\\epsilon_{t})+e^{\\alpha_{t}}\\epsilon_{t}=0\u0026令式(8.10)为零\\ \\ \u0026\\alpha_{t}+\\mathrm{ln}\\epsilon_{t}=-\\alpha_{t}+\\mathrm{ln}(1-\\epsilon_{t})\u0026移项后取对数 \\end{aligned} $$ 可解得： $$ \\alpha_{t}=\\dfrac{1}{2}\\mathrm{ln}\\Big(\\dfrac{1-\\epsilon_{t}}{\\epsilon_{t}}\\Big)\\tag{8.11} $$ 至此，我们求得了 AdaBoost 算法的分类器权重公式。\n样本分布更新公式 下面，我们推导 AdaBoost 算法的样本分布更新公式。\nAdaBoost 算法在获得 \\( H_{t-1} \\) 之后样本分布将进行调整，使下一轮的基学习器 \\( h_t \\) 能纠正 \\( H_{t-1} \\) 的一些错误。\n理想的 \\( h_t \\) 能纠正 \\( H_{t-1} \\) 的全部错误，由于 \\[ H(x) = H_{t-1}(x) + \\alpha_t h_t(x) \\] 因此，我们希望最小化 \\(\\mathscr{l}_{exp}(H_{t-1} + \\alpha_t h_t \\mid D)\\)，可以简化为，最小化： $$ \\begin{aligned} \\mathscr{l}_{exp}(H_{t-1} + h_t \\mid D) \u0026= \\mathbb{E}_{x \\sim D}[e^{-f(x)(H_{t-1}(x) + h_t(x))}] \\\\ \u0026= \\mathbb{E}_{x \\sim D}[e^{-f(x)H_{t-1}(x)} e^{-f(x)h_t(x)}] \\end{aligned} \\tag{8.12} $$对式（8.12）使用 \\( e^{-f(x)h_t(x)} \\) 的泰勒展式近似：\n使用到的泰勒展开式： \\( e^x \\sim 1 + x + \\dfrac{1}{2} x^{2} + o(x^2) \\)\n可得： $$ \\mathscr{l}_{exp}(H_{t-1} + h_t \\mid D) \\simeq \\mathbb{E}_{x \\sim D}\\Big[e^{-f(x)H_{t-1}(x)}\\Big(1 - f(x)h_t(x) + \\dfrac{f^2(x)h_t^2(x)}{2}\\Big)\\Big] $$又因为 \\( f^2(x) = h_t^2(x) = 1 \\)，可得： $$ \\mathscr{l}_{exp}(H_{t-1} + h_t \\mid D) = \\mathbb{E}_{x \\sim D}\\Big[e^{-f(x)H_{t-1}(x)}\\Big(1 - f(x)h_t(x) + \\dfrac{1}{2}\\Big)\\Big] \\tag{8.13} $$因为理想的基学习器 \\( h_t(x) \\) 要使得指数损失函数最小化，所以： $$ \\begin{aligned} h_t(x) \u0026= \\mathop{\\mathrm{argmin}}\\limits_{h} \\mathscr{l}_{exp}(H_{t-1} + h \\mid D) \\\\ \u0026= \\mathop{\\mathrm{argmin}}\\limits_{h} \\mathbb{E}_{x \\sim D}\\Big[e^{-f(x)H_{t-1}(x)}\\Big(1 - f(x)h(x) + \\dfrac{1}{2}\\Big)\\Big] \u0026 \\quad \\text{由式(8.13)代入得到} \\\\ \u0026= \\mathop{\\mathrm{argmin}}\\limits_{h} \\mathbb{E}_{x \\sim D}\\Big[e^{-f(x)H_{t-1}(x)}\\Big(-f(x)h(x)\\Big)\\Big] \u0026 \\\\ \u0026 \\text{常数 } 1 + \\dfrac{1}{2} \\text{ 不影响结果} \\\\ \u0026= \\mathop{\\mathrm{argmax}}\\limits_{h} \\mathbb{E}_{x \\sim D}\\Big[e^{-f(x)H_{t-1}(x)}f(x)h(x)\\Big] \u0026 \\text{去负号 } \\mathrm{argmin} \\text{ 变 } \\mathrm{argmax} \\\\ \u0026= \\mathop{\\mathrm{argmax}}\\limits_{h} \\mathbb{E}_{x \\sim D}\\Big[\\dfrac{e^{-f(x)H_{t-1}(x)}}{\\mathbb{E}_{x \\sim D}[e^{-f(x)H_{t-1}(x)}]}f(x)h(x)\\Big] \u0026 \\text{新加入的常数不影响结果} \\end{aligned} \\tag{8.14} $$注意，式（8.14）中的 \\( \\mathbb{E}_{x \\sim D}[e^{-f(x)H_{t-1}(x)}] \\) 是一个常数，之所以添加此常数，是为了构造出下面的 \\( D_t \\) 分布。\n令 \\( D_t \\) 表示一个分布： $$ D_t(x) = \\dfrac{D(x)e^{-f(x)H_{t-1}(x)}}{\\mathbb{E}_{x \\sim D}[e^{-f(x)H_{t-1}(x)}]} \\tag{8.15} $$根据数学期望的定义，再将式（8.15）代入式（8.14），则理想的基学习器 \\( h_t(x) \\)： $$ \\begin{aligned} h_t(x) \u0026= \\mathop{\\mathrm{argmax}}\\limits_{h}\\mathbb{E}_{x \\sim D}\\Big[\\dfrac{e^{-f(x)H_{t-1}(x)}}{\\mathbb{E}_{x \\sim D}[e^{-f(x)H_{t-1}(x)}]}f(x)h(x)\\Big] \u0026 \\text{式(8.14)} \\\\ \u0026= \\mathop{\\mathrm{argmax}}\\limits_{h}\\sum\\limits_{i=1}^{|D|}D(x_i)\\Big[\\dfrac{e^{-f(x_i)H_{t-1}(x_i)}f(x_i)h(x_i)}{\\mathbb{E}_{x \\sim D}[e^{-f(x_i)H_{t-1}(x_i)}]}\\Big] \u0026 \\text{数学期望的定义} \\\\ \u0026= \\mathop{\\mathrm{argmax}}\\limits_{h}\\sum_{i=1}^{|D|}D_t(x_i)f(x_i)h(x_i) \u0026 \\text{由式(8.15)代入得到} \\\\ \u0026= \\mathop{\\mathrm{argmax}}\\limits_{h}\\mathbb{E}_{x \\sim D_t}[f(x)h(x)] \\end{aligned} \\tag{8.16} $$由于 \\( f(x), h(x) \\in \\{-1, +1\\} \\)，有： $$ f(x)h(x) = 1 - 2\\mathbb{I}(f(x) \\neq h(x)) \\tag{8.17} $$则理想的基学习器 \\( h_t(x) \\)： $$ \\begin{aligned} h_t(x) \u0026= \\mathop{\\mathrm{argmax}}\\limits_{h}\\mathbb{E}_{x \\sim D_t}[f(x)h(x)] \u0026 \\text{式(8.16)} \\\\ \u0026= \\mathop{\\mathrm{argmax}}\\limits_{h}\\mathbb{E}_{x \\sim D_t}[1 - 2\\mathbb{I}(f(x) \\neq h(x))] \u0026 \\text{由式(8.17)代入得到} \\\\ \u0026= \\mathop{\\mathrm{argmin}}\\limits_{h}\\mathbb{E}_{x \\sim D_t}[\\mathbb{I}(f(x) \\neq h(x))] \u0026 \\text{去掉常数和负号} \\end{aligned} \\tag{8.18} $$由此可见，理想的 \\( h_t(x) \\) 将在分布 \\( D_t \\) 下最小化分类误差。\n因此，弱分类器将基于分布 \\( D_t \\) 进行训练，且针对 \\( D_t \\) 的分类误差应小于 0.5，这在一定程度上类似“残差逼近”的思想。\n考虑到 \\( D_t \\) 与 \\( D_{t+1} \\) 的关系，有： $$ \\begin{aligned} D_{t+1}(x) \u0026= \\dfrac{D(x)e^{-f(x)H_t(x)}}{\\mathbb{E}_{x \\sim D}[e^{-f(x)H_t(x)}]} \u0026 \\text{由式(8.15)可得} \\\\ \u0026= \\dfrac{D(x)e^{-f(x)H_{t-1}(x)} e^{-f(x)\\alpha_th_t(x)}}{\\mathbb{E}_{x \\sim D}[e^{-f(x)H_t(x)}]} \u0026 \\text{将 } H_t(x) \\text{ 展开得到} \\\\ \u0026= D_t(x)e^{-f(x)\\alpha_th_t(x)}\\dfrac{\\mathbb{E}_{x \\sim D}[e^{-f(x)H_{t-1}(x)}]}{\\mathbb{E}_{x \\sim D}[e^{-f(x)H_t(x)}]} \u0026 \\text{将式(8.15)代入，构造 } D_t(x) \\end{aligned} \\tag{8.19} $$ 至此，我们求得了\\(AdaBoost\\)算法的样本分布更新公式。\nXGBOOST Bagging 算法介绍 策略：\n从样本集中有放回(使得下次采样时还可选中)的采样出\\(m\\)个样本，重复操作，我们可以得到\\(T\\)个数量为\\(m\\)的训练样本集，然后基于每一个采样集训练出一个基学习器，再将这些基学习器进行结合，然后进行输出的预测。如果是分类任务，通常使用 简单投票法，对回归任务使用简单平均法。\n采样自助采样法的好处：初始训练集中约有63.2%的样本出现在采样集中，剩下的36.8%的样本可用作验证集。对于这部分大约36.8%的没有被采样到的数据，我们常常称之为袋外数据(\\(Out Of Bag, 简称OOB\\))。这些数据没有参与训练集模型的拟合，因此可以用来检测模型的泛化能力。\n算法流程 用较为通俗的话解释就是在每一轮训练中，通过自助采样获得样本集，使用这个样本集训练一个弱学习器，最后将所有的弱学习器进行结合。\n随机森林 随机森林(\\(Random Forest,简称RF\\))算法在\\(bagging\\)算法上进行了一点独特的改进，体现在以下几点。\n\\(1\\).\\(RF\\)以决策树为基学习器。\n\\(2\\).在训练过程中引入了随机属性选择。\n随机属性选择 传统决策树在选择划分属性是在当前结点的属性集合中选择一个最优属性。而\\(RF\\)有所不同，对基决策树的每个结点，先从该结点的属性集合(假设有\\(d\\)个属性)中随机选择一个包含\\(k\\)个属性的子集，然后再从这个子集中选择一个最优属性用于划分。 若\\(K=1\\)，相当于随机选择属性，\\(k=d\\)，则基决策树的构建与传统决策树相同，一般情况下，推荐\\(k=log2(d)\\)。\n算法 输入为样本集 \\(D={(x_1,y_1),(x_2,y_2),...(x_m,y_m)}\\)，弱分类器迭代次数\\(T\\)。输出为最终的强分类器\\(f(x)\\)　\\((1)\\)对于\\(t=1,2...,T\\):\n\\((a)\\)对训练集进行第\\(t\\)次随机采样，共采集\\(m\\)次，得到包含\\(m\\)个样本的采样集\\(D_t\\)\n\\((b)\\)用采样集\\(D_t\\)训练第t个决策树模型\\(G_t (x)\\)，在训练决策树模型的节点的时候， 在节点上所有的样本特征中选择一部分样本特征， 在这些随机选择的部分样本特征中选择一个最优的特征来做决策树的左右子树划分\n\\((2)\\)如果是分类算法预测，则\\(T\\)个弱学习器投出最多票数的类别或者类别之一为最终类别。如果是回归算法，\\(T\\)个弱学习器得到的回归结果进行算术平均得到的值为最终的模型输出。\n随机森林的优点 1.并行处理：由于每棵树的构建是相互独立的，随机森林可以很容易地实现并行化，提高训练效率。\n2.防止过拟合：随机森林通过引入随机性，降低了模型对训练数据的依赖，从而减少了过拟合的风险。\n3.高准确性：通过构建多个决策树并将结果进行投票或平均，随机森林能够提高模型的整体准确性。\n随机森林的缺点 1.在某些噪音比较大的特征上，\\(RF\\) 模型容易陷入过拟合；\n2.取值比较多的划分特征对\\(RF\\)的决策会产生更大的影响，从而有可能影响模型的效果。\n","date":"2024-10-20T15:19:41+08:00","permalink":"https://zhangherbal.github.io/p/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/","title":"机器学习-集成学习"},{"content":"线性表 线性表是由\\(n\\)个数据特性相同的元素构成的有限序列。\n线性表-顺序表 定义 线性表的顺序表示指的是用一组地址连续的存储单元依次存储线性表的数据元素。\n代码表示 1.结构定义\n1 2 3 4 5 #define MAXSIZE = 100 typedef struct{ ElemType *elem; int length; }Sqlist; 示例:图书管理系统涉及\n1 2 3 4 5 6 7 8 9 10 #define MAXSIZE = 100 typedef struct { string name; string ISBN; float price; }Book; typedef struct{ Book *elem; int length; }Sqlist; 类似定义后，可以直接通过变量定义语句\n1 sqlist l; 2.初始化(没有包括存储失败等)：\n1 2 3 4 Status Initlist(Sqlist \u0026amp;l){ l.elem = new Elemtype[Maxsize]; l.length = 0; } 3.取值\n取下标即可，判断下表是否合法即可\n4.查找\n平均复杂度为O(n)的查找，C语言基础，略。\n5.插入\n平均时间复杂度为O(n)的查找，C语言基础，略。\n6.删除\n平均时间复杂度为O(n)的删除操作，C语言基础，略。\n补充 因为线性表的顺序表示大部分为C语言基础，具体操作不再详写\n线性表-链表 定义与引出 线性表链式存储特点：用一组任意的存储单元存储线性表的数据元素(存储单元可以不连续)\n因为存储单元可以不连续，因此对于每个数据元素\\(a_i\\)来说，除了存储自己本身的信息，还要存储一个指示直接后继的信息。\n这两部分信息组成数据元素\\(a_i\\)的存储映像，称为节点。其中存储数据元素信息的域称为数据域，存储直接后继存储位置的域为指针域。\n许多节点链接成一个链表，即为线性表。\n线性表还有许多形式，如双链表等。\n基本操作实现+代码表示 定义 1 2 3 4 5 6 typedef struct { Elemtype data;//数据域 struct node *next;//指针域 }node,*Linklist; //其中Linklist是指向node的指针类型 1.Linklist和node 本质上是等价的，为了代码可读性，我们通常用Linklist定义单链表，强调其为某个单链表的头指针，用node定义指向单链表中任意节点的指针变量\n2.单链表是表头指针唯一确定的，因此单链表的名字可以用头指针的名字命名。\n3.若定义Linklist p或者Lnode p,则p为指向某节点的指针变量，表示该节点的地址，而p为对应的节点变量。\n初始化 1 2 3 4 5 Status Initlist(Sqlist \u0026amp;l){ //构造单链表L l = new node;//有头节点的，让单链表指向一个头节点 l-\u0026gt;next = NULL;//头节点的指针域为空 } 头节点： 在首元节点前的一个节点，指针域指向首元节点，头节点的数据域可以存储信息，也可以不存储\n头节点的用处：\n1.便于首元节点(链表中存储第一个数据元素的节点)处理，添加头节点后，对首元节点的操作跟其他节点一样，无需额外处理\n2.便于空表和非空表的统一管理。\n取值 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 //取节点序号为i的值 Status Get_elem(Linklist L,int i,ElemType \u0026amp;e){ p = L-\u0026gt;next; int j = 1; while(p\u0026amp;\u0026amp;j\u0026lt;i){ p = p-\u0026gt;next; ++j; } if(!p||j\u0026gt;i) return Error; e = p-\u0026gt;data; return OK; } #### 查找 ```C++ //在带头节点的单链表L中查找值为e的元素 node *LocateELem(Linklist L,Elemtype e){ p = L-\u0026gt;next; while(p\u0026amp;\u0026amp;p-\u0026gt;data!=e){ p = p-\u0026gt;next; } return p; } #### 插入 ```C++ Status ListInsert(Linklist \u0026amp;l,int i,Elemtype e){ //在带头节点的单链表L的第i个位置插入值为e的点 p = L; int j = 0; while(p\u0026amp;\u0026amp;j\u0026lt;i-1){ p = p-\u0026gt;next; j++; }//先找到第i-1个 if(!p||j\u0026gt;i-1) return ERROR; s = new node; s-\u0026gt;data = e; s-\u0026gt;next = p-\u0026gt;next; p-\u0026gt;next = s; return OK; } 删除 1 2 3 4 5 6 7 8 9 10 11 12 13 14 Status ListDelete(Linklist \u0026amp;l,int i){ //在带头节点的单链表L中，删除第i个元素 p = l; int j = 0; while(p\u0026amp;\u0026amp;j\u0026lt;(i-1)){ p = p-\u0026gt;next; j++; } if(!p||j\u0026gt;i-1) return ERROR; q= p-\u0026gt;next; p-\u0026gt;next = q-\u0026gt;next; delete q; return OK; } 创建单链表 前插法 后插法 循环链表 ","date":"2024-10-15T18:46:53+08:00","permalink":"https://zhangherbal.github.io/p/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84-%E7%BA%BF%E6%80%A7%E8%A1%A8/","title":"数据结构-线性表"},{"content":"支持向量机 前言 本篇文章对B站一些高质量视频，看过的高质量文章，西瓜书等著作进行白话总结\n支持向量机-线性模型 知识补充 补充1:超平面的定义 超平面是指\\(n\\)维线性空间中维度为\\(n-1\\)的子空间。它可以把线性空间分割成不相交的两部分。比如二维空间中，一条直线是一维的，它把平面分成了两块；三维空间中，一个平面是二维的，它把空间分成了两块。\n补充2：拉格朗日乘子法 在本章只给出基本介绍，具体推导及原理将单开一章进行阐述。 拉格朗日乘子法是用于求解有约束的优化问题的数学方法，本章涉及的流程为\n\\(1\\).构造拉格朗日函数\n\\(2\\).解变量的偏导方程\n\\(3\\).代入目标函数即可\n\\(4\\).\\(KKT\\)条件\n若约束为不等式约束就要满足\\(KKT\\)条件：在约束\\(g(x)\u003c=0\\)下最小化\\(f(x)\\),可转化为在如下约束下最小化拉格朗日函数： 设 \\( a \\) 为拉格朗日乘子，\\(KKT\\)条件为：\n\\[ g(x) \\leq 0, \\quad a \\geq 0, \\quad a \\cdot g(x) = 0 \\]问题引出 支持向量机可以用作分类算法\n分类算法的思想：给定训练样本集 \\( D = \\{ (x_1, y_1),(x_2,y_2),...(x_m,y_m) \\} \\) ,\\(y_i∈\\{-1,+1\\},\\) ，基于训练集\\(D\\)在样本空间中找到一个划分超平面，将不同类别的样本分开，但是有很多超平面都能将训练样本分开，因此我们要找到一个最优的超平面完成分类任务。\n训练样本集分为线性可分训练集和非线性可分训练集，线性可分样本集意味着我们可以用一个超平面把样本集分成两个部分\n图中共有五个超平面对我们的样本集进行划分，在这五个超平面中哪一个是最优秀的超平面呢?通过观察可以发现，超平面\\(3\\)对误差容忍性最好，为什么呢？为了验证误差的容忍性，我们首先假设存在误差，数据集中的向量的位置可能发生错误，观察图中的箭头，平面\\(1\\)和平面\\(5\\)无法容忍向量发生误差导致变化(因为导致了分类错误)，通过观察可以发现其余平面(特别偏向某一侧数据点)对误差的容忍性相对较弱，非常容易出现分类错误。而平面\\(3\\)受到的影响最小，因此我们选择了超平面\\(3\\)，那么我们该如何去描述我们刚刚决策的过程呢？\n我们要定义一个性能指标来帮助我们去寻找这个超平面。在下图中我们将\\(1\\)号超平面向两侧进行平移，直到擦到两侧数据集中的圆圈和叉叉停下，这也说明了被擦中的向量离\\(1\\)号超平面最近，我们称这些向量为支持向量\\((support vector)\\)。停下后此时又生成了平面\\(2\\)和平面\\(3\\)，我们将平面\\(2\\)和平面\\(3\\)之间的距离\\(d\\)称为间隔\\((margin)\\)，官方一点我们也可以称之为两个异类支持向量到超平面的距离，我们将间隔作为性能指标，我们要做的事情就是将\\(d\\)最大化，除此之外，我们会发现一个问题，跟这个超平面平行的超平面其实都能做到让\\(margin\\)最大化，因此它们都能实现到平面\\(2\\)和平面\\(3\\)的距离之和为\\(d\\),因此我们设置限定条件，即这个超平面到平面\\(2\\)和平面\\(3\\)的距离都是\\(\\frac{d}{2}\\)。 接下来我们就进入数学世界来进行推导。\n数学描述 定义 定义1：训练数据及标签 \\( D = \\{ (x_1, y_1),(x_2,y_2),...(x_m,y_m) \\} \\) ,\\(y_i∈\\{-1,+1\\},\\)，\\(x_i是多维向量,y_i是标签\\)，\\(y_i\\)通常取\\(+1\\)或者\\(-1\\)。\n定义2： 超平面方程 超平面方程为\\(w^Tx+b=0\\),其中\\(w\\)也是向量，与\\(x\\)的维度相同。\n定义3： 线性模型的任务 即确定超平面方程中的\\(w\\)和\\(b\\)，确定最终的超平面方程。\n定义4：训练集的线性可分 对于\\( (x_i, y_i), \\, i \\in \\{1, 2, \\ldots, N\\} \\)，\\(\\exists(w,b)\\)，使\\(\\forall i \\in \\{1, 2, \\ldots, N\\}\\)有\n\\(a\\).若\\(y_i=+1\\)，则\\(w^Tx+b≥0\\)\n\\(b\\).若\\(y_i=-1\\)，则\\(w^Tx+b\u003c0\\)\n将上述两式进行结合可得 \\(y_i(w^Tx+b) \\geq 0\\) \\((1)\\)\n推导过程 条件1： \\(w^Tx+b=0\\) 与 \\(aw^Tx+ab=0\\)是同一个平面，\\(a\\)是正整数，如果前者满足公式\\((1)\\)，后者也满足。\n条件2： 平面 \\(w_1x+w_2y+b=0\\)\n则\\((x_0,y_0)\\)到此平面距离d的值为 \\( d= \\frac{|w_1 x_0 + w_2 y_0 + b|}{\\sqrt{w_1^2 + w_2^2}}\\)\n向量\\(x_0\\)到超平面\\(w^Tx+b=0\\)的距离为\\( \\frac{|w^T x_0 + b|}{\\|w\\|}\\)\n推导开始 假设支持向量为\\(x_0\\)，我们的任务是最大化间隔\\((margin)\\)，也就是最大化\\(d\\)，\\(d\\)由支持向量到超平面距离公式可得，但是观察公式我们貌似无法解出值，此时我们让\\(w^T x_0+b=1\\)，那么支持向量所在超平面方程变为\\(w^T x_0+b=1\\)和\\(w^T x_0+b=-1\\)且距离直接就变成了\\( \\frac{2}{\\|w\\|}\\)，但是凭啥呢？ 假设\\(d_0=|w^T x_0+b|\\)，根据条件1，同时缩放\\(w,b\\)后平面不变，因此得到\\(d_0=|a(w^T x_0+b)|\\)，a可以通过取值使\\(w^T x_0+b=1\\)。因为要最大化间隔，因此只要最小化分母即可。\n除此之外，我们发现终极目标中的限制条件也发生了变化，支持向量所在平面都是\\(w^T x_0+b=1\\)，那么其他的向量确定的平面肯定是大于等于\\(1\\)，乘以\\(y_i\\)是为了保证\\((1)\\)的成立。\n终极目标 式\\((2)\\)\n最小化 \\((minimise)\\)\n\\( \\|w\\|^2 \\)\n限制条件\\((subject\\ to)\\)\n\\(y_i (w^T x_i + b) \\geq 1, \\quad \\forall i \\in \\{1, 2, \\ldots, N\\}\\) 优化方法 拉格朗日乘子法 终极目标其实是一个二次规划问题，也是凸优化问题，要么无解要么有一个极小值。 我们采用拉格朗日乘子法进行解决。对式\\((2)\\)的每条约束添加拉格朗日乘子 \\(α_i\u003e=0\\),可得出拉格朗日函数.\n\\(L(w,b,α)=\\frac{1}{2}\\|w\\|^2+ \\sum_{i=1}^{m} α_i(1-y_i(w^Tx_i+b))\\quad(3)\\)\n令拉格朗日函数对\\(w,b\\)求偏导为0(计算过程如下) 得出\n\\(w=\\sum_{i=1}^{m}a_iy_ix_i\\quad(4)\\)\n\\(0=\\sum_{i=1}^{m}a_iy_i\\quad(5)\\)\n将\\((3),(4),(5)\\)结合消去\\(w,b\\)引出对偶问题\n$$ \\begin{align*} \\text{maximize} \u0026 \\quad \\sum_{i=1}^{m} \\alpha_i - \\frac{1}{2} \\sum_{i=1}^{m} \\sum_{j=1}^{m} \\alpha_i \\alpha_j y_i y_j (x_i^T \\cdot x_j) \\\\ \\text{subject to} \u0026 \\quad \\sum_{i=1}^{m} \\alpha_i y_i = 0, \\\\ \u0026 \\quad \\alpha_i \\geq 0 \\quad \\forall i\\in \\{1, 2, \\ldots, m\\}. \\end{align*} $$ 因为是不等式约束于是还要满足\\(KKT\\)条件:\n\\(α\u003e=0,y_if(x_i)-1\u003e=0,α_i(y_if(x_i)-1)=0\\)\n通过观察\\(α_i(y_if(x_i)-1)=0\\)得出结论要么\\(α_i=0\\)，要么\\(y_if(x_i)=1\\)，又因为\\(|y_i|=1\\)得出\\(|f(x)|=1\\),也就是满足\\(α_i\u003e0\\)的点即为支持向量，我们在式\\((4)\\)中已经得出了\\(w\\),若知道支持向量\\((x_k,y_k)\\)代回即可得出 \\(b=y_k-\\sum_{i=1}^{m}α_ix_i^Ty_ix_k\\)\n支持向量机-非线性模型 模型修正 如果训练集样本是非线性可分样本集，那么我们按照线性模型进行操作就无法找出对应的\\(w,b\\)。因此我们要对模型进行改进。\n修正1 为了避免过拟合以及保证更优越的鲁棒性，我们引入松弛变量和正则项进行调和。在很多场合也被称为软间隔\\(SVM\\)。\n最小化 \\((minimise)(7)\\)\n\\(\\frac{1}{2} \\|w\\|^2 + C \\sum_{i=1}^N \\xi_i\\)\n限制条件\\((subject\\ to)(8)\\)\n\\(\\quad y_i (w^T x_i + b) \\geq 1 - \\xi_i, \\quad \\forall i \\in \\{1, 2, \\ldots, N\\}\\)\n\\(\\xi_i \\geq 0, \\quad \\forall i \\in \\{1, 2, \\ldots, N\\}\\)\n通过观察我们可以发现，如果\\(\\xi_i\\)足够大，那么式\\((8)\\)一定会得到满足，而我们在式\\((7)\\)中最小化目标函数又保证了\\(\\xi_i\\)不会特别大。在式\\((7)\\)中\\(C\\)是预设好的未知常数，需要提前设置，具体值要根据实践确定。我们通常称\\(\\xi_i\\)为松弛变量， \\(C \\sum_{i=1}^N \\xi_i\\)为正则项。\n修正2 我们做到第一步修正其实还不够，如下图，真正能成功分类样本的其实是一个椭圆，但是我们的模型决定了它是一条直线或者是超平面。因此我们需要将低维的x向高维映射，这样线性可分的概率就变大。\n高维映射\n\\(x-\u003e\\phi(x)\\)\n最小化 \\((minimise)(7)\\)\n\\(\\frac{1}{2} \\|w\\|^2 + C \\sum_{i=1}^N \\xi_i\\)\n限制条件\\((subject\\ to)(8)\\)\n\\(\\quad y_i (w^T \\phi(x_i) + b) \\geq 1 - \\xi_i, \\quad \\forall i \\in \\{1, 2, \\ldots, N\\}\\)\n\\(\\xi_i \\geq 0, \\quad \\forall i \\in \\{1, 2, \\ldots, N\\}\\)\n现在的\\(w\\)维度也将发生变化。\n核函数 那么我们该如何去选取这个\\(\\phi(x)\\)呢，根据\\(SVM\\)的原理，我们假设\\(\\phi(x)\\)是无限维，那么我们发现我们的问题将会变得不可做，那么该如何解决这个问题呢。 SVM精妙之处将被引出： 我们可以不知道\\(\\phi(x)\\)的显式表达，但是我们可以通过核函数(\\(Kernal\\ function\\))把这个问题变得可做。\n\\(k(x1,x2)=\\phi(x_1)^T \\phi(x_2)\\)(后面两者相乘明显是一个数值)\n线性核: \\[ K(x, y) = x^T y=\\phi(x_1)^T \\phi(x_2) \\] 多项式核: \\[ K(x, y) = (x^T y + c)^d=\\phi(x_1)^T \\phi(x_2) \\] 高斯（RBF）核: \\[ K(x, y) = \\exp\\left(-\\frac{\\|x - y\\|^2}{2\\sigma^2}\\right)=\\phi(x_1)^T \\phi(x_2) \\] Sigmoid核: \\[ K(x, y) = \\tanh(\\alpha x^T y + c)=\\phi(x_1)^T \\phi(x_2) \\]\n充要条件 \\(k(x_1,x_2)=\\phi(x_1)^T \\phi(x_2)\\)的充要条件是：(推导暂无)\n交换性：\\(k(x_1,x_2)=k(x_2,x_1)\\)\n半正定性:\\(\\forall c_i\u003e0,x_i\u003e0 (\\sum_{i=1}^N \\sum_{j=1}^N c_ic_jk(x_1,x_2)\u003e=0) \\) \\(\\quad \\forall i \\in \\{1, 2, \\ldots, N\\}\\)\n我们最常用的就是高斯核，我们可以通过继续转化问题来使用核函数去解决这个问题，把这个优化问题变得可做。\n数学推导 拉格朗日乘子法 此问题还是二次规划问题，与线性模型的推导方法相同。 因为存在两个限制条件，因此引入两个拉格朗日乘子，\\(a_i\u003e=0,u_i\u003e=0\\),那么拉格朗日函数为\n\\[ \\mathcal{L}(\\mathbf{w}, b, \\boldsymbol{\\alpha}, \\boldsymbol{\\xi},u) = \\frac{1}{2} \\|\\mathbf{w}\\|^2 + C \\sum_{i=1}^{m} \\xi_i - \\sum_{i=1}^{m} \\alpha_i \\left( y_i (\\mathbf{w}^T \\phi(x_i) + b) - 1 + \\xi_i \\right) - \\] \\(\\sum_{i=1}^{n} \\mu_i \\xi_i\\)\n对 \\(\\mathbf{w}\\) 求导并设置为零： \\[ \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{w}} = \\mathbf{w} - \\sum_{i=1}^{m} \\alpha_i y_i \\phi(x_i) = 0 \\] 得到： \\[ \\mathbf{w} = \\sum_{i=1}^{m} \\alpha_i y_i \\phi(x_i) \\] 对 \\(b\\) 求导并设置为零： \\[ \\frac{\\partial \\mathcal{L}}{\\partial b} = -\\sum_{i=1}^{m} \\alpha_i y_i = 0 \\] 对 \\(\\xi_i\\) 求导并设置为零： \\[ \\frac{\\partial \\mathcal{L}}{\\partial \\xi_i} = C - \\alpha_i - \\mu_i = 0 \\] 联立上述式子代回拉格朗日函数得到最终结果(推导过程跟线性基本一样) \\[ \\mathcal{L}(\\boldsymbol{\\alpha}) = \\sum_{i=1}^{m} \\alpha_i - \\frac{1}{2} \\sum_{i=1}^{m} \\sum_{j=1}^{m} \\alpha_i \\alpha_j y_i y_j \\phi(\\mathbf{x}_i)^T \\phi(\\mathbf{x}_j) \\] 将核函数代入得到 \\[ \\mathcal{L}(\\boldsymbol{\\alpha}) = \\sum_{i=1}^{m} \\alpha_i - \\frac{1}{2} \\sum_{i=1}^{m} \\sum_{j=1}^{m} \\alpha_i \\alpha_j y_i y_j k(x_i,x_j) \\] 至此引出对偶问题 最大化: \\[ \\mathcal{L}(\\boldsymbol{\\alpha}) = \\sum_{i=1}^{m} \\alpha_i - \\frac{1}{2} \\sum_{i=1}^{m} \\sum_{j=1}^{m} \\alpha_i \\alpha_j y_i y_j k(x_i,x_j) \\] 限制条件: \\(0 \\leq a_i \\leq c\\)\\(\\quad \\forall i \\in \\{1, 2, \\ldots, m\\}\\)\n\\( \\sum_{i=1}^{m} \\alpha_i y_i = 0\\) 现在我们\\(y_i,y_j,k(x_i,x_j)\\)都能求出，只剩\\(α\\)的值，这还是一个凸优化问题，我们要SMO算法进行解决，还有其他的一些算法，可以选择一种去学习。 以上就是SVM非线性模型的理论内容。\n测试过程 我们发现我们无法直接求出\\(w\\),但我们只要知道分类完毕的结果就好\n给出测试样本\\(X\\) 若\\(w^T(\\phi(x))+b \\geq 0\\),则 \\(y=1\\)。 若\\(w^T(\\phi(x))+b \u003c 0\\),则 \\(y=-1\\)。\n\\(w^T\\phi(x)=\\sum_{i=1}^{N}α_iy_i \\phi(x_i)^T\\phi(x)\\)\n\\(w^T\\phi(x)=\\sum_{i=1}^{N}α_iy_i k(x_1,x)\\)\n根据\\(KKT\\)条件求出\\(b\\)的值即可，类似线性模型推导\n","date":"2024-10-06T14:27:00+08:00","permalink":"https://zhangherbal.github.io/p/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/","title":"机器学习-支持向量机"},{"content":"","date":"2024-10-05T16:22:55+08:00","permalink":"https://zhangherbal.github.io/p/newnew/","title":"Newnew"},{"content":"","date":"2024-10-04T16:59:26+08:00","permalink":"https://zhangherbal.github.io/p/myfirstblog/","title":"Myfirstblog"},{"content":"正文测试 而这些并不是完全重要，更加重要的问题是， 带着这些问题，我们来审视一下学生会退会。 既然如何， 对我个人而言，学生会退会不仅仅是一个重大的事件，还可能会改变我的人生。 我们不得不面对一个非常尴尬的事实，那就是， 可是，即使是这样，学生会退会的出现仍然代表了一定的意义。 学生会退会，发生了会如何，不发生又会如何。 经过上述讨论， 生活中，若学生会退会出现了，我们就不得不考虑它出现了的事实。 学生会退会，到底应该如何实现。 这样看来， 在这种困难的抉择下，本人思来想去，寝食难安。 对我个人而言，学生会退会不仅仅是一个重大的事件，还可能会改变我的人生。 就我个人来说，学生会退会对我的意义，不能不说非常重大。 莎士比亚曾经提到过，人的一生是短的，但如果卑劣地过这一生，就太长了。这似乎解答了我的疑惑。 莫扎特说过一句富有哲理的话，谁和我一样用功，谁就会和我一样成功。这启发了我， 对我个人而言，学生会退会不仅仅是一个重大的事件，还可能会改变我的人生。 学生会退会，到底应该如何实现。 一般来说， 从这个角度来看， 这种事实对本人来说意义重大，相信对这个世界也是有一定意义的。 在这种困难的抉择下，本人思来想去，寝食难安。 了解清楚学生会退会到底是一种怎么样的存在，是解决一切问题的关键。 一般来说， 生活中，若学生会退会出现了，我们就不得不考虑它出现了的事实。 问题的关键究竟为何？ 而这些并不是完全重要，更加重要的问题是。\n奥斯特洛夫斯基曾经说过，共同的事业，共同的斗争，可以使人们产生忍受一切的力量。　带着这句话，我们还要更加慎重的审视这个问题： 一般来讲，我们都必须务必慎重的考虑考虑。 既然如此， 这种事实对本人来说意义重大，相信对这个世界也是有一定意义的。 带着这些问题，我们来审视一下学生会退会。 我认为， 我认为， 在这种困难的抉择下，本人思来想去，寝食难安。 问题的关键究竟为何？ 每个人都不得不面对这些问题。 在面对这种问题时， 要想清楚，学生会退会，到底是一种怎么样的存在。 我认为， 既然如此， 每个人都不得不面对这些问题。 在面对这种问题时， 那么， 我认为， 学生会退会因何而发生。\n引用 思念是最暖的忧伤像一双翅膀\n让我停不了飞不远在过往游荡\n不告而别的你 就算为了我着想\n这么沉痛的呵护 我怎么能翱翔\n最暖的憂傷 - 田馥甄\n图片 1 2 3 ![Photo by Florian Klauer on Unsplash](florian-klauer-nptLmg6jqDo-unsplash.jpg) ![Photo by Luca Bravo on Unsplash](luca-bravo-alS7ewQ41M8-unsplash.jpg) ![Photo by Helena Hertz on Unsplash](helena-hertz-wWZzXlDpMog-unsplash.jpg) ![Photo by Hudai Gayiran on Unsplash](hudai-gayiran-3Od_VKcDEAA-unsplash.jpg) 相册语法来自 Typlog\n","date":"2020-09-09T00:00:00Z","image":"https://zhangherbal.github.io/p/test-chinese/helena-hertz-wWZzXlDpMog-unsplash_hu4699868770670889127.jpg","permalink":"https://zhangherbal.github.io/p/test-chinese/","title":"Chinese Test"},{"content":"This article offers a sample of basic Markdown syntax that can be used in Hugo content files, also it shows whether basic HTML elements are decorated with CSS in a Hugo theme.\nHeadings The following HTML \u0026lt;h1\u0026gt;—\u0026lt;h6\u0026gt; elements represent six levels of section headings. \u0026lt;h1\u0026gt; is the highest section level while \u0026lt;h6\u0026gt; is the lowest.\nH1 H2 H3 H4 H5 H6 Paragraph Xerum, quo qui aut unt expliquam qui dolut labo. Aque venitatiusda cum, voluptionse latur sitiae dolessi aut parist aut dollo enim qui voluptate ma dolestendit peritin re plis aut quas inctum laceat est volestemque commosa as cus endigna tectur, offic to cor sequas etum rerum idem sintibus eiur? Quianimin porecus evelectur, cum que nis nust voloribus ratem aut omnimi, sitatur? Quiatem. Nam, omnis sum am facea corem alique molestrunt et eos evelece arcillit ut aut eos eos nus, sin conecerem erum fuga. Ri oditatquam, ad quibus unda veliamenimin cusam et facea ipsamus es exerum sitate dolores editium rerore eost, temped molorro ratiae volorro te reribus dolorer sperchicium faceata tiustia prat.\nItatur? Quiatae cullecum rem ent aut odis in re eossequodi nonsequ idebis ne sapicia is sinveli squiatum, core et que aut hariosam ex eat.\nBlockquotes The blockquote element represents content that is quoted from another source, optionally with a citation which must be within a footer or cite element, and optionally with in-line changes such as annotations and abbreviations.\nBlockquote without attribution Tiam, ad mint andaepu dandae nostion secatur sequo quae. Note that you can use Markdown syntax within a blockquote.\nBlockquote with attribution Don\u0026rsquo;t communicate by sharing memory, share memory by communicating.\n— Rob Pike1\nTables Tables aren\u0026rsquo;t part of the core Markdown spec, but Hugo supports supports them out-of-the-box.\nName Age Bob 27 Alice 23 Inline Markdown within tables Italics Bold Code italics bold code A B C D E F Lorem ipsum dolor sit amet, consectetur adipiscing elit. Phasellus ultricies, sapien non euismod aliquam, dui ligula tincidunt odio, at accumsan nulla sapien eget ex. Proin eleifend dictum ipsum, non euismod ipsum pulvinar et. Vivamus sollicitudin, quam in pulvinar aliquam, metus elit pretium purus Proin sit amet velit nec enim imperdiet vehicula. Ut bibendum vestibulum quam, eu egestas turpis gravida nec Sed scelerisque nec turpis vel viverra. Vivamus vitae pretium sapien Code Blocks Code block with backticks 1 2 3 4 5 6 7 8 9 10 \u0026lt;!doctype html\u0026gt; \u0026lt;html lang=\u0026#34;en\u0026#34;\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026#34;utf-8\u0026#34;\u0026gt; \u0026lt;title\u0026gt;Example HTML5 Document\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;p\u0026gt;Test\u0026lt;/p\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; Code block indented with four spaces \u0026lt;!doctype html\u0026gt; \u0026lt;html lang=\u0026quot;en\u0026quot;\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026quot;utf-8\u0026quot;\u0026gt; \u0026lt;title\u0026gt;Example HTML5 Document\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;p\u0026gt;Test\u0026lt;/p\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; Code block with Hugo\u0026rsquo;s internal highlight shortcode 1 2 3 4 5 6 7 8 9 10 \u0026lt;!doctype html\u0026gt; \u0026lt;html lang=\u0026#34;en\u0026#34;\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026#34;utf-8\u0026#34;\u0026gt; \u0026lt;title\u0026gt;Example HTML5 Document\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;p\u0026gt;Test\u0026lt;/p\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; Diff code block 1 2 3 4 5 [dependencies.bevy] git = \u0026#34;https://github.com/bevyengine/bevy\u0026#34; rev = \u0026#34;11f52b8c72fc3a568e8bb4a4cd1f3eb025ac2e13\u0026#34; - features = [\u0026#34;dynamic\u0026#34;] + features = [\u0026#34;jpeg\u0026#34;, \u0026#34;dynamic\u0026#34;] List Types Ordered List First item Second item Third item Unordered List List item Another item And another item Nested list Fruit Apple Orange Banana Dairy Milk Cheese Other Elements — abbr, sub, sup, kbd, mark GIF is a bitmap image format.\nH2O\nXn + Yn = Zn\nPress CTRL + ALT + Delete to end the session.\nMost salamanders are nocturnal, and hunt for insects, worms, and other small creatures.\nHyperlinked image The above quote is excerpted from Rob Pike\u0026rsquo;s talk during Gopherfest, November 18, 2015.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","date":"2019-03-11T00:00:00Z","image":"https://zhangherbal.github.io/p/markdown-syntax-guide/pawel-czerwinski-8uZPynIu-rQ-unsplash_hu6307248181568134095.jpg","permalink":"https://zhangherbal.github.io/p/markdown-syntax-guide/","title":"Markdown Syntax Guide"},{"content":"Lorem est tota propiore conpellat pectoribus de pectora summo.\nRedit teque digerit hominumque toris verebor lumina non cervice subde tollit usus habet Arctonque, furores quas nec ferunt. Quoque montibus nunc caluere tempus inhospita parcite confusaque translucet patri vestro qui optatis lumine cognoscere flos nubis! Fronde ipsamque patulos Dryopen deorum.\nExierant elisi ambit vivere dedere Duce pollice Eris modo Spargitque ferrea quos palude Rursus nulli murmur; hastile inridet ut ab gravi sententia! Nomine potitus silentia flumen, sustinet placuit petis in dilapsa erat sunt. Atria tractus malis.\nComas hunc haec pietate fetum procerum dixit Post torum vates letum Tiresia Flumen querellas Arcanaque montibus omnes Quidem et Vagus elidunt The Van de Graaf Canon\nMane refeci capiebant unda mulcebat Victa caducifer, malo vulnere contra dicere aurato, ludit regale, voca! Retorsit colit est profanae esse virescere furit nec; iaculi matertera et visa est, viribus. Divesque creatis, tecta novat collumque vulnus est, parvas. Faces illo pepulere tempus adest. Tendit flamma, ab opes virum sustinet, sidus sequendo urbis.\nIubar proles corpore raptos vero auctor imperium; sed et huic: manus caeli Lelegas tu lux. Verbis obstitit intus oblectamina fixis linguisque ausus sperare Echionides cornuaque tenent clausit possit. Omnia putatur. Praeteritae refert ausus; ferebant e primus lora nutat, vici quae mea ipse. Et iter nil spectatae vulnus haerentia iuste et exercebat, sui et.\nEurytus Hector, materna ipsumque ut Politen, nec, nate, ignari, vernum cohaesit sequitur. Vel mitis temploque vocatus, inque alis, oculos nomen non silvis corpore coniunx ne displicet illa. Crescunt non unus, vidit visa quantum inmiti flumina mortis facto sic: undique a alios vincula sunt iactata abdita! Suspenderat ego fuit tendit: luna, ante urbem Propoetides parte.\n","date":"2019-03-09T00:00:00Z","image":"https://zhangherbal.github.io/p/placeholder-text/matt-le-SJSpo9hQf7s-unsplash_hu10664154974910995856.jpg","permalink":"https://zhangherbal.github.io/p/placeholder-text/","title":"Placeholder Text"},{"content":"Mathematical notation in a Hugo project can be enabled by using third party JavaScript libraries.\nIn this example we will be using KaTeX\nCreate a partial under /layouts/partials/math.html Within this partial reference the Auto-render Extension or host these scripts locally. Include the partial in your templates like so: 1 2 3 {{ if or .Params.math .Site.Params.math }} {{ partial \u0026#34;math.html\u0026#34; . }} {{ end }} To enable KaTeX globally set the parameter math to true in a project\u0026rsquo;s configuration To enable KaTeX on a per page basis include the parameter math: true in content files Note: Use the online reference of Supported TeX Functions\nExamples Inline math: $\\varphi = \\dfrac{1+\\sqrt5}{2}= 1.6180339887…$\nBlock math: $$ \\varphi = 1+\\frac{1} {1+\\frac{1} {1+\\frac{1} {1+\\cdots} } } $$","date":"2019-03-08T00:00:00Z","permalink":"https://zhangherbal.github.io/p/math-typesetting/","title":"Math Typesetting"},{"content":"Emoji can be enabled in a Hugo project in a number of ways.\nThe emojify function can be called directly in templates or Inline Shortcodes.\nTo enable emoji globally, set enableEmoji to true in your site\u0026rsquo;s configuration and then you can type emoji shorthand codes directly in content files; e.g.\n🙈 :see_no_evil: 🙉 :hear_no_evil: 🙊 :speak_no_evil:\nThe Emoji cheat sheet is a useful reference for emoji shorthand codes.\nN.B. The above steps enable Unicode Standard emoji characters and sequences in Hugo, however the rendering of these glyphs depends on the browser and the platform. To style the emoji you can either use a third party emoji font or a font stack; e.g.\n1 2 3 .emoji { font-family: Apple Color Emoji, Segoe UI Emoji, NotoColorEmoji, Segoe UI Symbol, Android Emoji, EmojiSymbols; } ","date":"2019-03-05T00:00:00Z","image":"https://zhangherbal.github.io/p/emoji-support/the-creative-exchange-d2zvqp3fpro-unsplash_hu5876398126655421130.jpg","permalink":"https://zhangherbal.github.io/p/emoji-support/","title":"Emoji Support"}]