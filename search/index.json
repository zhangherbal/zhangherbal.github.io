[{"content":"时序差分学习 基本概念及其背景 ​\t时序差分算法\\(（Temporal Difference, TD）\\)是一种强化学习方法，结合了蒙特卡洛方法和动态规划的优点，用于估计状态值函数。时序差分方法也可以直接从环境互动的经验中学习策略，而不需要构建关于环境动态特性的模型。\n自举思想 ​\t自举意味着当前状态的价值估计依赖于未来状态的估计值，而不是直接依赖真实的最终回报。这种方式使得\\(TD\\)方法在不需要完整回报（如蒙特卡洛方法）的情况下，仍能进行学习和更新。\n在线学习 ​\t在线学习是指在强化学习中，智能体在与环境交互的过程中，实时更新自己的策略和价值函数。在这个过程中，智能体不需要等待所有数据收集完毕，而是通过每一步的反馈（奖励和状态转移），在学习过程中逐步改进其行为。也就是说模型在接收到数据后立即进行更新。\n离线学习 ​\t离线学习是指智能体在一个固定的数据集（通常是收集到的历史数据或模拟数据）上进行学习，而不依赖实时的环境交互过程。离线学习更适合大规模的训练。\n同轨策略 ​\t在同轨策略的强化学习中，学习过程中使用的策略是与行为策略（即选择动作的策略）相同的。也就是说，智能体在环境中执行动作的策略和它用来学习的策略是一致的。\n离轨策略 ​\t在离轨策略的强化学习中，学习策略和行为策略是不同的。智能体在执行动作时使用的策略（行为策略）与用于学习和更新价值函数的策略是分开的。\nTD TD算法引出 与蒙特卡洛的区别 ​\t在谈到TD算法时就不得忽视蒙特卡洛算法，毕竟TD算法是结合了蒙特卡洛方法和动态规划方法的优点。\n特性 时序差分（TD） 蒙特卡洛（MC） 更新方式 每个时间步都更新 只有在回合结束后更新 是否使用自举（Bootstrapping） 是（依赖自身估计的值） 否（依赖完整回报） 收敛速度 更快（每步更新） 更慢（需要完整回合） 数据需求 适用于持续任务，不需要终止状态 必须等待回合终止 偏差与方差 有偏估计（Bias），但方差较小 无偏估计（Unbiased），但方差较大 计算效率 高（单步更新） 较低（需要回合结束） 适用场景 适用于长期任务、在线学习 适用于回合制任务、离线学习 $$ V(S_t) = V(S_t) + \\alpha[G_t-V(S_t)] $$$$ G_t: 时刻t的真实回报 \\\\ \\alpha:常量步长参数 \\\\ V():状态价值函数 $$ 蒙特卡洛方法必须等到一幕的末尾才能确定对\\( V(S_t) \\)的增量，因为这时\\(G_t \\) 才是已知的,而\\(TD\\)方法只需要等到下一个时刻就行。\nTD(0)算法 首先我们引出第一个概念\\(TD\\)误差：\n$$ \\delta_t = R_{t+1} + \\gamma V(S_{t+1}) - V(S_t) $$ 其中，\\(R_{t+1}\\) 是在时刻 \\(t+1\\) 获得的奖励，\\(\\gamma\\) 是折扣因子，\\(V(S_t)\\) 和 \\(V(S_{t+1})\\) 分别是时刻 \\(t\\) 和 \\(t+1\\) 时状态 \\(S_t\\) 和 \\(S_{t+1}\\) 的价值估计。\n接着我们给出更新规则：\n$$ V(S_t) \\leftarrow V(S_t) + \\alpha \\delta_t $$ 其中，\\(\\alpha\\) 是学习率，控制每次更新的步长。由此就得到了\\(TD(0)\\)的算法框架。\nTD(0)的最优性 \\(Sarsa\\) : 同轨策略下的时序差分控制 是一种同轨策略\\(（On-Policy）\\)的强化学习算法。它通过实时与环境的交互，学习一个行为策略，并根据该策略更新Q值（状态-动作价值函数），从而使智能体在环境中获得最大回报。接下来我们将给出其工作原理：\n首先我们要学习的是动作价值函数而不是状态价值函数，对于同轨策略方法，我们必须对所有状态\\(S\\)以及动作\\(a\\)，估计出在当前的行动策略下所有对应的\\(q(s,a)\\)\n\\(Sarsa\\) 算法的核心思想是根据当前选择的动作和下一状态下选择的动作来更新\\(Q\\)值。它是一个在线学习算法，通过与环境的交互来不断更新状态-动作价值函数。在\\(TD(0)\\)算法中我们学习了状态之间的转移以及状态的价值。在\\(Sarsa\\)中我们讨论的是状态-动作二元组之间的转移，在数学形式上这两者是相同的，他们都是带有收益的马尔科夫链，因此\\(TD（0）\\)的收敛定理也适用于\\(Sarsa\\)，因此我们可以引出更新公式。\n更新公式 首先定义一个五元组\\((S_t,A_t，S_{t+1},A_{t+1}，R_{t+1})\\)\n$$ Q(S_t, A_t) \\leftarrow Q(S_t, A_t) + \\alpha \\left[ R_{t+1} + \\gamma Q(S_{t+1}, A_{t+1}) - Q(S_t, A_t) \\right] $$ 其中：\n\\(S_t\\) 是当前状态。 \\(A_t\\) 是在状态\\(S_t\\)下选择的动作。 \\(R_{t+1}\\) 是执行动作\\(A_t\\)后得到的奖励。 \\(S_{t+1}\\) 是执行动作\\(A_t\\)后的下一个状态。 \\(A_{t+1}\\) 是在状态\\(S_{t+1}\\)下选择的下一个动作。 \\(\\gamma\\) 是折扣因子，表示未来奖励的权重。 \\(\\alpha\\) 是学习率，表示每次更新的步长。 \\(Sarsa\\)的算法流程如下： Q学习：离轨策略下的时序差分控制 \\(Q\\)学习\\(（Q-learning）\\)是一种基于 离轨策略\\(（Off-Policy）\\) 的强化学习算法，旨在学习一个最优策略，通过不断与环境交互，估计每个状态-动作对的价值（Q值）。\nQ学习的更新公式 $$ Q(S_t, A_t) \\leftarrow Q(S_t, A_t) + \\alpha \\left[ R_{t+1} + \\gamma \\max_a Q(S_{t+1}, a) - Q(S_t, A_t) \\right] $$ 其中：\n\\(S_t\\)：当前状态。\n\\(A_t\\)：在状态 $S_t$ 下选择的动作。\n\\(R_{t+1}\\)：执行动作 $A_t$ 后获得的即时奖励。\n\\(S_{t+1}\\)：执行动作后到达的下一个状态。\n\\(\\max_a Q(S_{t+1}, a)\\)：状态\\(S_{t+1}\\) 下所有可能动作的最大Q值，代表目标策略（即最优策略）。\n\\(\\gamma\\)：折扣因子，用于权衡当前奖励和未来奖励的影响。\n\\(\\alpha\\)：学习率，表示每次更新的步长。\nQ学习的算法流程 期望Sarsa 期望\\(Sarsa\\)是对\\(Sarsa\\)算法的扩展，它与\\(Q\\)学习十分类似，但把对于下一个“状态-动作”二元组取最大值这一步换成取期望，使得更新不依赖于实际采取的动作，而是基于在给定状态下所有可能动作的期望\\(Q\\)值。具体来说，期望\\(Sarsa\\)考虑了在下一个状态中所有可能动作的加权平均值，权重由行为策略给出。\n期望Sarsa的更新公式 $$ Q(S_t, A_t) \\leftarrow Q(S_t, A_t) + \\alpha \\left[ R_{t+1} + \\gamma \\sum_{a'} \\pi(a'|S_{t+1}) Q(S_{t+1}, a') - Q(S_t, A_t) \\right] $$ 其中：\n\\(\\pi(a'|S_{t+1})\\) 是行为策略下在状态 \\(S_{t+1}\\)下选择动作\\(a'\\)的概率。 \\(\\sum_{a'} \\pi(a'|S_{t+1}) Q(S_{t+1}, a')\\) 表示在状态\\(S_{t+1}\\)下，所有可能动作的Q值的期望值（加权平均）。 ​\t与\\(Sarsa\\)不同的是，在期望\\(Sarsa\\)中，更新不仅仅依赖于下一个状态的实际选择的动作，而是依赖于所有可能的动作的期望\\(Q\\)值，这使得期望\\(Sarsa\\)在一定程度上更加稳定。\n双Q学习 双\\(Q\\)学习\\(（Double Q-learning)\\)是强化学习中的一种方法，旨在克服传统\\(Q\\)学习\\(（Q-learning）\\)中常见的过估计问题。传统Q学习算法在更新\\(Q\\)值时，通常会遇到过估计的问题，即在更新\\(Q\\)值时，算法会倾向于高估最优动作的价值，尤其是在动作值较为接近的情况下。这个问题会导致学习过程中策略不稳定，收敛速度变慢。\n双\\(Q\\)学习通过引入两个独立的Q值函数来缓解这一问题，从而提高\\(Q\\)学习算法的性能。\n传统Q学习的过估计问题 $$ Q(S_t, A_t) \\leftarrow Q(S_t, A_t) + \\alpha \\left[ R_{t+1} + \\gamma \\max_{a'} Q(S_{t+1}, a') - Q(S_t, A_t) \\right] $$ 传统\\(Q\\)学习存在过估计问题，也叫最大化偏差问题，\n双Q学习的核心思想 双\\(Q\\)学习通过两个\\(Q\\)值函数来减轻过估计问题。在双\\(Q\\)学习中，算法维护两个\\(Q\\)值函数\\(Q_1\\)和\\(Q_2\\)，并通过交替更新这两个\\(Q\\)值函数来避免过度依赖单一的\\(Q\\)值估计。具体来说，更新过程中，不再直接使用一个\\(Q\\)值函数来选择和评估最优动作，而是使用两个独立的\\(Q\\)值函数交替进行。\n双Q学习的更新公式 $$ Q_1(S_t, A_t) \\leftarrow Q_1(S_t, A_t) + \\alpha \\left[ R_{t+1} + \\gamma Q_2(S_{t+1}, \\arg\\max_{a'} Q_1(S_{t+1}, a')) - Q_1(S_t, A_t) \\right] $$ 在这个更新公式中：\n\\(Q_1(S_t, A_t)\\) 和 \\(Q_2(S_t, A_t)\\) 是两个独立的Q值函数。 \\(\\arg\\max_{a'} Q_1(S_{t+1}, a')\\) 是根据\\(Q_1\\)选择的最优动作。 然后，使用\\(Q_2\\)来评估该动作的价值。 通过这种方式，双\\(Q\\)学习避免了传统\\(Q\\)学习中的过估计问题，因为每次更新时，评估最优动作价值的\\(Q\\)值函数与选择最优动作的\\(Q\\)值函数是不同的。 算法流程 ","date":"2025-03-12T18:55:16+08:00","permalink":"https://zhangherbal.github.io/p/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0-%E6%97%B6%E5%BA%8F%E5%B7%AE%E5%88%86%E5%AD%A6%E4%B9%A0/","title":"强化学习-时序差分学习"},{"content":"多臂赌博机问题 K臂赌博机问题 问题引出：一个赌博机有好几个臂，我们可以选择一些臂获得一些收益，但是我们事先并不知道这些臂的收益是多少。玩家希望摇固定次数的臂所获得的期望累计奖励最大\n原始形式：重复地在\\(K\\)个选项或动作中进行选择，每次做出选择之后，都会得到一定数值的收益，收益由你选择的动作决定的平稳概率分布产生，目标是在某一段时间内最大化总收益的期望。\n相关定义及问题引出 价值 价值：\\(K\\)个动作中的每个动作在被选择时都有一个期望或者平均收益，我们称此为这个动作的价值，在时刻t选择的动作\\(A_t\\),并将对应的收益即为\\(R_t\\),任一动作\\(a\\)对应的价值记作\\(q_{*}(a)\\) ,是给定动作\\(a\\)时收益的期望。\n\\(q_{*}(a)=E[R_t|A_t=a]\\)\n类比赌博机\n行为：摇哪个臂\n奖励：每次摇臂获得的奖金\n\\(A_t\\)表示第\\(t\\)轮的行为，对应的收益为\\(R_t\\)\n第\\(t\\)轮采取动作\\(a\\)的期望奖励即为\\(q_{*}(a)\\)\n每一个选择对应的奖励为\\(q_{*}(a)\\)。因此我们首先要完成一个任务才能去思考这件事情：对动作\\(a\\)在时刻\\(t\\)时的价值进行估计。只有估计出这个值我们才能继续进行我们的选择策略。这个值为\\(Q_{*}(a)\\)\n玩家在第\\(t\\)轮时只能根据估计的\\(Q_{*}(a)\\)来进行选择，如果按照贪心策略，那么只需要每轮选择\\(Q_{*}(a)\\)最大的行为即可。\n那么估计的效果也很重要，估计的好，最后获得的奖励值可能也越大。 接着我们就对估计的过程进行讲述。\n估计过程 估计过程：持续对动作的价值进行估计去选择动作。\n贪心：对应最高估计价值的动作。\n试探：选择非贪心的动作。\n缺点：短期奖励很低\n优点：长期奖励可能会高。可能找出奖励更大的行为\n开发：选择贪心的动作。\n缺点：当前估计值最高不一定意味着其\\(q_{*}(a)\\)最高。\n优点：短期奖励很高。\n通过试探和开发来看，我们可以感知到，开发是当下最好的，但是试探可能对未来长远更好。接下来我们就引入估计的方法。\n动作价值方法 我们接下来将给出估计动作价值的方法。即估计在\\(t\\)时刻选择动作\\(A\\)的价值。\n\\(Q_{*}(a)=\\frac{t时刻前通过执行动作a得到的收益总和}{t时刻前执行动作a的次数}\\)\n\\(=\\frac{\\sum_{i=1}^{t-1} R_i*\\mathbb{I}A_i=a}{\\sum_{i=1}^{t-1}\\mathbb{I}A_i=a}\\)\n当分母为0时我们将\\(Q_{*}(a)\\)定义为某个默认值，比如\\(Q_{*}(a)=0\\)。当分母趋向无穷大时，根据大数定律，\\(Q_t(a)\\)会收敛到\\(q_{*}(a)\\),这种采样方法称为采样平均方法。 至此我们就可以初步估计出动作的值。这种方法其实就是我们回顾我们探索的过程，把历史上所有探索过的取一下均值。\n补充-大数定律 \\(1\\).弱大数定理：设\\(x_1,x_2……\\)是相互独立，服从同一分布的随机变量序列且具有数学期望\\(E(x_k)=u(k=1,2……)\\),作前\\(n\\)个变量的算数平均\n\\(\\lim_{n \\to \\infty} \\{ |\\frac{1}{n} \\sum_{k=1}^{n} (x_k-u)|\u003c\\epsilon \\}=1\\)\n白话翻译是：当n很大时，他们的算术平均近似可以看作为\\(u\\)\n开发-试探平衡 至此，我们已经解决了估计动作价值的问题，那么在估计完成后，我们要思考，怎么进行选择呢。最简单是我们每一轮都选最大的，但是每一轮都选最大的不一定是最优的。也有可能我们去试探，能试探出更优的行为，因此我们要平衡和开发和试探。 我们将引出一些方法。\n\\(\\epsilon\\)贪心方法 以一个很小的概率\\(\\epsilon\\)去做试探，那么次数无限多的情况下，每个动作都会被采样无限次，那么估计值将遵循大数定律逼近均值\\(q_{*}(a)\\)\n举例 假设有\\(10\\)个臂，每个臂的期望奖励从一个均值为\\(0\\)，方差为\\(1\\)的正态分布中得到。在第\\(t\\)轮中选择行为\\(A_t\\)获得奖励服从均值为\\(q_{*}(A_t)\\),方差为\\(1\\)的正态分布。 上图为10个动作的收益分布。 改进-增量式实现 在我们使用\\(\\epsilon\\)贪心方法的时候，如果我们按照前文的估计方法，要把历史的奖励都加起来，然后除以轮数。时间复杂度较高，因此我们要对此进行一点小小的优化。\n令\\(R_i\\)表示这一动作被选择\\(i\\)次后获得的收益，\\(Q_n\\)表示被选择\\(n-1\\)次后它的估计的动作价值，可以写为\n\\(Q_n=\\frac{R_1+R_2+……R_{n-1}}{n-1}\\),但是每计算一次，我们需要消耗很多内存，计算量也很大，因此我们要对其优化。\n\\(Q_{n+1}=\\frac{1}{n} \\sum_{i=1}^{n} R_i\\)\n\\(=\\frac{1}{n}(R_n+\\sum_{i=1}^{n-1}R_i)\\)\n\\(=\\frac{1}{n}(R_n+(n-1)\\frac{1}{n-1}\\sum_{i=1}^{n-1}R_i)\\)\n\\(=\\frac{1}{n}(R_n+(n-1)Q_n\\))\n\\(=\\frac{1}{n}(R_n+nQ_n-Q_n\\))\n\\(=Q_n+\\frac{1}{n}[R_n-Q_n]\\)\n本式也被称为更新公式，它的一般形式为:\n新估计值\u0026lt;-旧估计值+步长*[目标-旧估计值]\n通过观察发现，我们只需要存储\\(Q_n\\)和\\(n\\)即可。 （目标-旧估计值）是估计值的误差，误差会随着向目标靠近的每一步而减小。上述方法的步长会随着时间而变化，处理动作\\(a\\)对应的第\\(n\\)个收益的方法用的步长是\\(\\frac{1}{n}\\),在本书中我们将步长记作\\(α\\),或者为\\(α_t(a)\\). 至此，我们完成了一个简单的多臂赌博机算法，但是如果赌博机的收益的概率是随着时间变化的该怎么办呢?用一个通俗的例子进行说明，一个学生小学三年级数学期末考试考30分，高一数学期末考试考了120分，我们无法认为75分就是下次的估计值。 定义补充 首先我们要搞明白两个概念。\n1.平稳问题：\n固定的奖励分布，每个动作的奖励分布在时间上保持不变，例如在多臂赌博机中，某个臂的期望奖励是固定的。随着观测样本的增加，平均值估计方法最终收敛于\\(q_{*}(a)\\)\n2.非平稳问题：\n每个动作的奖励分布随着时间变化，例如某个臂某段时间内表现好，某段时间表现差，因此机制复杂。也就是说\\(q_{*}(a)\\)是关于时间的函数，我们更关注最近的样本。\n多臂赌博机-非平稳问题 从平稳问题的视角来看，某个臂的期望奖励是固定的，而从非平稳问题的视角来看，赌博机的收益概率随时间变化，取平均方法不可用，那我们是如何改进的呢？\n基础-非平稳下的行为估值\n我们可以引入\\(α\\)，使每个行为的权值都不再相等。\n\\(Q_{n+1}=Q_n+α[R_n-Q_n]\\) \\(α \u0026in; [0,1]\\)\n\\(Q_{n+1}\\)成为对过去的收益和初始的估计\\(Q_1\\)的加权平均\n\\(Q_{n+1}=Q_n+α[R_n-Q_n]\\)\n=\\(αR_n+(1-α)Q_n\\)\n=\\(αR_n+(1-α)[αR+{n-1}+(1-α)Q_{n-1}]\\)然后进行不断拆分\n=\\((1-α)^nQ_1+\\sum_{i=1}^{n}α(1-α)^{n-i}R_i\\)\n\\(Q_{n+1}\\)成为对过去的收益和初始的估计\\(Q_1\\)的加权平均\n之所以可以成为加权平均是因为：\\((1-α)+\\sum_{i=1}^{n}α(1-α)^{n-i}=1\\) 因此我们随着时刻一步一步改变步长参数，设\\(α_n(a)\\)表示用于处理第\\(n\\)次选择动作\\(a\\)后收到的收益的步长参数\n当我们选择\\(α\\)时，会发现最终结果不是收敛的，而是随着近期得到的收益而变化,而之前选用的平均估值法的\\(\\frac{1}{n}\\)是收敛的，这个也符合我们对非平稳问题的预期。 因此在选择步长参数时要观察其是否收敛。\n收敛条件 \\(\\sum_{n=1}^{\\infty}a_n(α)=\\infty\\)且\\(\\sum_{n=1}^{\\infty}a_n(α)^2\u003c\\infty\\)\n第一个条件：要求保证有足够大的步长。最终克服任何初始条件或随机波动\n第二个条件：保证最终步长变小，以保证收敛。\n乐观初值法 我们首先观察行为的初始估值。\n\\(1\\).在我们之前的贪心策略中，每个行为的初始估值为0。\n\\(2\\).每个行为的初始估值可以帮助我们引入先验知识。\n\\(3\\).初始估值还可以帮助我们平衡试探和开发。\n乐观初值法是什么意思呢，比如目前有十个臂，我们替换掉原先的初始值0，换为+5，在之前我们的问题描述中，\\(q_{*}(a)\\)是按照均值为0，方差为1的正态分布去进行选择的，因此+5是一个非常乐观的估计，但是这种乐观的估计会鼓励 动作-价值方法去试探。因为无论选哪一种动作，都比开始的估计值小，因此学习器会失望去主动进行试探，结果是所有动作都被探索了很多次。乐观初值法也可以理解为鼓励探索。\n同时我们也会发现，乐观初值法比较适合平稳问题，不太适合非平稳问题。毕竟非平稳问题更关注最近的样本。\n\\(UCB\\)行为选择策略 \\[ a_t = \\arg\\max_{i \\in \\{1, 2, \\dots, K\\}} \\left( Q_t(a) + c \\sqrt{\\frac{\\ln t}{N_t(a)}} \\right) \\] \\(N_t(a)\\)表示时间\\(t\\)之前行为\\(a\\)被选择的次数。参数\\(c\\)决定了置信水平。 此公式引入了置信度，例如一个臂之前只被摇了两次，那么它的预测值就不太准，如果被摇了几万次，那就比较准确。因此平方根项是对\\(a\\)动作值估计的不确定性和方差的度量，随着时间的变化， 具有较低价值估计的动作或者已经被选择了更多次的动作被选择的频率较低。\n","date":"2024-11-09T14:00:53+08:00","permalink":"https://zhangherbal.github.io/p/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0-%E5%A4%9A%E8%87%82%E8%B5%8C%E5%8D%9A%E6%9C%BA/","title":"强化学习-多臂赌博机"},{"content":"集成学习 知识补充 弱学习器 在给定任务中，其性能略好于随机猜测的学习模型，但是显著弱于强学习器。\n强学习器 在给定任务中，性能远超随机猜测的学习模型。它能够有效地从数据中学习并做出准确的预测，通常具有较高的准确率和较好的泛化能力。\n串行 下一个分类器只在前一个分类器预测不够准确的实例上进行训练或者检验。\n并行 所有的弱分类器都给出各自的预测结果，通过组合把这些预测结果转化为最终结果。\n定义 集成学习是使用一系列学习器进行学习，并使用某种规则把各个学习结果进行整合从而获得比单个学习器更好的学习效果的一种机器学习方法。\n集成学习通过将多个学习器进行结合，可获得比单一学习器显著卓越的泛化性能，对弱学习器最明显，因此集成学习的很多理论学习都是针对弱学习器进行的。\n相关问题 问题1：弱学习器之间的关系 第一种就是所有个体学习器都是一个种类的，也可以称为同质的。例如决策树的集成全是决策树，神经网络集成中全是神经网络。\n第二种就是所有个体学习器不全是一个种类的，也可以称为异质的。例如同时包含决策树和神经网络。\n其中同质个体学习器按照个体学习器之间的依赖关系又分两类。一类是当一系列个体学习器存在强依赖关系，都要串行生成，代表为\\(Boosting\\)系列算法，第二类是个体学习器之间不存在强依赖关系，一系列个体学习器可以并行生成，代表算法是\\(bagging\\)和随机森林。\n问题2：学习器的选择 我们在选择学习器时要秉持好而不同的原则。既要考虑准确性，也要考虑多样性。\n准确性：是指个体学习器不能太差，要有一定的准确度。\n多样性：是指个体学习器之间的输出要有差异性。\n简单举例，下图是一个二分类任务的结果，对号表示分类正确。集成结果少数服从多数。 问题3：怎样组合弱学习器 假定集成包含T个基学习器\\(\\{ h_1,h_2,……h_T \\}\\)，其中\\(h_i\\)在示例\\(x\\)上的输出为\\(h_i(x)\\)。\n平均法 对数值型输出，数值类的回归预测问题比较常用。\n简单平均法 \\[ H(x) = \\frac{1}{T} \\sum_{i=1}^{T} h_i(x) \\]加权平均法 \\[ H(x) = \\sum_{i=1}^{T} w_i \\cdot h_i(x) \\] 其中 \\( w_i \\) 是第 \\( i \\) 个个体学习器的权重，满足 \\( \\sum_{i=1}^{T} w_i = 1，w_i \\geq 0 \\)。\n投票法 对于分类问题比较常见。\n常见的投票方法有绝对多数投票法(若某标记记得票数超过半数，则预测为该标记)，相对多数投票法(预测为得票最多的标记，若有多个标记获得最高票，则从中随机选取)，加权投票法等。\n学习法 当训练数据很多时，我们可以采取学习法，即通过另一个学习器来进行结合，\\(Stacking\\)是学习法的典型代表，我们将个体学习器称为初级学习器，用于结合的学习器称为元学习器。\n步骤如下\n\\(1\\).将训练数据集划分为多个子集，通常是两个或更多个。\n\\(2\\).对于每个子集，使用不同的初级学习器进行训练和预测，得到每个初级学习器的预测结果。\n\\(3\\).将这些预测结果作为新的特征，组合成一个新的训练数据集。\n\\(4\\).使用这个新的训练数据集来训练一个元学习器，例如逻辑回归、决策树等。\n\\(5\\).最后，使用训练好的元学习器来对测试数据进行预测。\nBoosting 算法介绍 Boosting是可以将弱学习器提升为强学习器的算法：先从初始训练集训练出一个基学习器，再根据基学习器的表现对训练样本分布进行调整，使得先前基学习器做错的训练样本在后续收到更多关注，然后基于调整后的样本分布来训练下一个基学习器，如此反复进行，直至 基学习器数目达到事先指定的值T，最终将这T个基学习器进行加权结合。\n即采用了重赋权法：即在训练过程的每一轮中，根据样本分布为每一个训练样本重新赋予一个权重。对无法接受带权样本的 基学习算法，则可以通过重采样法来处理，即在每一轮的学习中，根据样本分布对训练集重新进行采样，在对重采样而来的样本集对基学习器进行训练。 算法补充 1.准确率越高的弱学习机权重越高。\nAdaboost \\(Adaboost\\)全称为\\((adaptive \\ boosting)\\),\\(adaptive\\)的意思是自适应，那么与\\(boosting\\)相比，\\(Adaboost\\)是如何去实现自适应的呢？\\(Adaboost\\)需要解决两个问题。\n1.在每一轮训练中，如何对训练样本的权值分布进行调整？\n2.如何确定各个基学习器“加权”结合的权重？\nAdaBoost的解决思路 通过分类器权重公式来确定各个基学习器的权重；通过样本分布更新公式来确定各轮训练中的样本分布。\n1.在确定各个基学习器的权重时：\n加大“分类误差率”小的弱分类器的权值，使其在表决中发挥较大作用；减小“分类误差率”大的弱分类器的权值，使其在表决中发挥较小作用。\n2.在确定各轮训练中的样本分布时：\n提高那些被前一轮弱分类器错误分类样本的权值，降低那些被正确分类的样本的权值。\n算法流程 Adaboost公式推导 推导前置说明 $$ P(h_i(x)\\neq f(x))=\\epsilon \\tag{8.1} $$$$ F_{简单投票}(x)=\\mathrm{sign}\\Big(\\sum\\limits_{i=1}^{T}h_i(x)\\Big) \\tag{8.2} $$$$ H(x)=\\sum\\limits_{t=1}^T\\alpha_{t}h_{t}(x) \\tag{8.4} $$$$ F(x)=\\mathrm{sign}(H(x))=\\mathrm{sign}\\Big(\\sum\\limits_{t=1}^T\\alpha_{t}h_{t}(x)\\Big) $$$$ \\mathscr{l}_{exp}(H\\mid D)=\\mathbb{E}{x\\sim D}[e^{-f(x)H(x)}]\\tag{8.5} $$ 其中，\\(x\\sim D\\) 表示概率分布 D(x)，\\(\\mathbb{E}\\) 表示期望。\n这个指数损失函数为什么合理：当对于某一学习器而言，\\(Adaboost\\)的优点就是给予其权重，我们注意到\\(e\\)的指数为\\(-H(x)*f(x)\\)，当对于某一样本\\(x\\)，预测正确时 \\(H(x)\\)和\\(f(x)\\)应该为同号，前面加上符号，指数就变成了负数，同时若\\(H(x)\\)的值越大(分类器对结果的信任程度就高)，整个函数的值就越小。同时若分类错误，\\(H(x)\\)与\\(f(x)\\) 为异号，那么\\(e\\)的指数变为一个正数，且\\(H(x)\\)越大，整个损失函数的值都会变大(分类器错付了，那么损失函数值就要反应出问题)。\n在搞懂前提后，我们还需要借助两个公式。\n\\( \\mathbb{E}_{x\\sim D}[f(x)]=\\sum_{x\\in D}D(x)f(x)\\tag{工具公式1} \\) \\(\\sum\\limits_{i=1}^{|D|}D(x_i)\\mathbb{I}(f(x_i)=1)=p(f(x_i)=1|x_i)\\tag{工具公式2}\\)\n损失函数正确性说明 $$ \\dfrac{\\partial\\mathscr{l}_{exp}(H\\mid D)}{\\partial H(x)}=-e^{-H(x)}P(f(x)=1\\mid x)+e^{H(x)}P(f(x)=-1\\mid x)\\tag{8.6} $$$$ \\mathscr{l}_{exp}(H\\mid D)=\\mathbb{E}{x\\sim D}[e^{-f(x)H(x)}]=\\sum\\limits_{x\\in D}D(x)e^{-f(x)H(x)} $$$$ \\sum\\limits_{i=1}^{|D|}D(x_i)\\mathbb{I}(f(x_i)=1)=P(f(x_i)=1\\mid x_i) $$$$ \\begin{aligned} \\dfrac{\\partial \\mathscr{l}_{exp}(H \\mid D)}{\\partial H(x)} \u0026= \\sum\\limits_{i=1}^{|D|} D(x_i) \\Big( -e^{-H(x_i)} \\mathbb{I}(f(x_i)=1) + e^{H(x_i)} \\mathbb{I}(f(x_i)=-1) \\Big) \\end{aligned}\n$$$$ -e^{-H(x)} P(f(x)=1 \\mid x) + e^{H(x)} P(f(x)=-1 \\mid x) = 0 $$\\[ H(x) + \\ln(P(f(x)=-1 \\mid x)) = -H(x) + \\ln(P(f(x)=1 \\mid x) \\] 可解得：\n$$ H(x) = \\dfrac{1}{2} \\ln \\dfrac{P(f(x)=1 \\mid x)}{P(f(x)=-1 \\mid x)} \\tag{8.7} $$因此，有：\n$$ \\begin{aligned} \\mathrm{sign}(H(x)) \u0026= \\mathrm{sign}\\Big(\\dfrac{1}{2} \\ln \\dfrac{P(f(x)=1 \\mid x)}{P(f(x)=-1 \\mid x)}\\Big) \\\\ \u0026= \\begin{cases} 1, \u0026 P(f(x)=1 \\mid x) \u003e P(f(x)=-1 \\mid x) \\\\ -1, \u0026 P(f(x)=1 \\mid x) \u003c P(f(x)=-1 \\mid x) \\end{cases} \\\\ \u0026= \\mathop{\\mathrm{argmax}}\\limits_{y \\in \\{-1, 1\\}} P(f(x)=y \\mid x) \\end{aligned} \\tag{8.8} $$ 这意味着 \\(\\mathrm{sign}(H(x))\\) 达到了贝叶斯最优错误率（即对于每个样本\\(x\\) 都选择后验概率最大的类别）。换言之，若指数损失函数最小化，则分类错误率也将最小化；这说明指数损失函数是分类任务原本 0/1 损失函数的一致的\\(（consistent）\\)替代损失函数。由于这个替代损失函数有更好的数学性质，例如它是连续可微函数，因此我们用它替代 0/1 损失函数作为优化目标。 至此，我们知道了使用“指数损失函数”的原因及其正确性。\n分类器权重公式 $$ \\begin{aligned} \\mathscr{l}_{exp}(\\alpha_t h_t \\mid D_t) \u0026= \\mathbb{E}_{x \\sim D_t}[e^{-f(x) \\alpha_t h_t(x)}] \\end{aligned} $$$$ \\begin{aligned} \u0026=\\mathbb{E}_{x \\sim D_t}\\left[e^{-\\alpha_t} \\mathbb{I}(f(x) = h_t(x)) + e^{\\alpha_t} \\mathbb{I}(f(x) \\neq h_t(x))\\right] \\end{aligned} $$$$ \\begin{aligned} \u0026=\\sum\\limits_{x \\in D_t} D_t(x) \\left[e^{-\\alpha_t} \\mathbb{I}(f(x) = h_t(x)) + e^{\\alpha_t} \\mathbb{I}(f(x) \\neq h_t(x))\\right] \\end{aligned} $$$$ \\begin{aligned} \u0026= e^{-\\alpha_t} P_{x \\sim D_t}(f(x) = h_t(x)) + e^{\\alpha_t} P_{x \\sim D_t}(f(x) \\neq h_t(x)) \\end{aligned} $$$$ \\begin{aligned} \u0026= e^{-\\alpha_t}(1 - \\epsilon_t) + e^{\\alpha_t} \\epsilon_t \\quad \\text{错误率的定义推出} \\end{aligned} \\tag{8.9} $$$$ \\begin{aligned} \u0026\\dfrac{\\partial\\mathscr{l}_{exp}(\\alpha_th_t\\mid D_{t})}{\\partial\\alpha_t}=-e^{-\\alpha_t}(1-\\epsilon_t)+e^{\\alpha_t}\\epsilon_{t} \u0026由式(8.9)对\\alpha_{t}求偏导 \\end{aligned}\\tag{8.10} $$$$ \\begin{aligned} \u0026-e^{-\\alpha_{t}}(1-\\epsilon_{t})+e^{\\alpha_{t}}\\epsilon_{t}=0\u0026令式(8.10)为零\\ \\ \u0026\\alpha_{t}+\\mathrm{ln}\\epsilon_{t}=-\\alpha_{t}+\\mathrm{ln}(1-\\epsilon_{t})\u0026移项后取对数 \\end{aligned} $$$$ \\alpha_{t}=\\dfrac{1}{2}\\mathrm{ln}\\Big(\\dfrac{1-\\epsilon_{t}}{\\epsilon_{t}}\\Big)\\tag{8.11} $$ 至此，我们求得了 AdaBoost 算法的分类器权重公式。\n样本分布更新公式 下面，我们推导 AdaBoost 算法的样本分布更新公式。\nAdaBoost 算法在获得 \\( H_{t-1} \\) 之后样本分布将进行调整，使下一轮的基学习器 \\( h_t \\) 能纠正 \\( H_{t-1} \\) 的一些错误。\n\\[ H(x) = H_{t-1}(x) + \\alpha_t h_t(x) \\]$$ \\begin{aligned} \\mathscr{l}_{exp}(H_{t-1} + h_t \\mid D) \u0026= \\mathbb{E}_{x \\sim D}[e^{-f(x)(H_{t-1}(x) + h_t(x))}] \\\\ \u0026= \\mathbb{E}_{x \\sim D}[e^{-f(x)H_{t-1}(x)} e^{-f(x)h_t(x)}] \\end{aligned} \\tag{8.12} $$对式（8.12）使用 \\( e^{-f(x)h_t(x)} \\) 的泰勒展式近似：\n使用到的泰勒展开式： \\( e^x \\sim 1 + x + \\dfrac{1}{2} x^{2} + o(x^2) \\)\n$$ \\mathscr{l}_{exp}(H_{t-1} + h_t \\mid D) \\simeq \\mathbb{E}_{x \\sim D}\\Big[e^{-f(x)H_{t-1}(x)}\\Big(1 - f(x)h_t(x) + \\dfrac{f^2(x)h_t^2(x)}{2}\\Big)\\Big] $$$$ \\mathscr{l}_{exp}(H_{t-1} + h_t \\mid D) = \\mathbb{E}_{x \\sim D}\\Big[e^{-f(x)H_{t-1}(x)}\\Big(1 - f(x)h_t(x) + \\dfrac{1}{2}\\Big)\\Big] \\tag{8.13} $$$$ \\begin{aligned} h_t(x) \u0026= \\mathop{\\mathrm{argmin}}\\limits_{h} \\mathscr{l}_{exp}(H_{t-1} + h \\mid D) \\\\ \u0026= \\mathop{\\mathrm{argmin}}\\limits_{h} \\mathbb{E}_{x \\sim D}\\Big[e^{-f(x)H_{t-1}(x)}\\Big(1 - f(x)h(x) + \\dfrac{1}{2}\\Big)\\Big] \u0026 \\quad \\text{由式(8.13)代入得到} \\\\ \u0026= \\mathop{\\mathrm{argmin}}\\limits_{h} \\mathbb{E}_{x \\sim D}\\Big[e^{-f(x)H_{t-1}(x)}\\Big(-f(x)h(x)\\Big)\\Big] \u0026 \\\\ \u0026 \\text{常数 } 1 + \\dfrac{1}{2} \\text{ 不影响结果} \\\\ \u0026= \\mathop{\\mathrm{argmax}}\\limits_{h} \\mathbb{E}_{x \\sim D}\\Big[e^{-f(x)H_{t-1}(x)}f(x)h(x)\\Big] \u0026 \\text{去负号 } \\mathrm{argmin} \\text{ 变 } \\mathrm{argmax} \\\\ \u0026= \\mathop{\\mathrm{argmax}}\\limits_{h} \\mathbb{E}_{x \\sim D}\\Big[\\dfrac{e^{-f(x)H_{t-1}(x)}}{\\mathbb{E}_{x \\sim D}[e^{-f(x)H_{t-1}(x)}]}f(x)h(x)\\Big] \u0026 \\text{新加入的常数不影响结果} \\end{aligned} \\tag{8.14} $$注意，式（8.14）中的 \\( \\mathbb{E}_{x \\sim D}[e^{-f(x)H_{t-1}(x)}] \\) 是一个常数，之所以添加此常数，是为了构造出下面的 \\( D_t \\) 分布。\n$$ D_t(x) = \\dfrac{D(x)e^{-f(x)H_{t-1}(x)}}{\\mathbb{E}_{x \\sim D}[e^{-f(x)H_{t-1}(x)}]} \\tag{8.15} $$$$ \\begin{aligned} h_t(x) \u0026= \\mathop{\\mathrm{argmax}}\\limits_{h}\\mathbb{E}_{x \\sim D}\\Big[\\dfrac{e^{-f(x)H_{t-1}(x)}}{\\mathbb{E}_{x \\sim D}[e^{-f(x)H_{t-1}(x)}]}f(x)h(x)\\Big] \u0026 \\text{式(8.14)} \\\\ \u0026= \\mathop{\\mathrm{argmax}}\\limits_{h}\\sum\\limits_{i=1}^{|D|}D(x_i)\\Big[\\dfrac{e^{-f(x_i)H_{t-1}(x_i)}f(x_i)h(x_i)}{\\mathbb{E}_{x \\sim D}[e^{-f(x_i)H_{t-1}(x_i)}]}\\Big] \u0026 \\text{数学期望的定义} \\\\ \u0026= \\mathop{\\mathrm{argmax}}\\limits_{h}\\sum_{i=1}^{|D|}D_t(x_i)f(x_i)h(x_i) \u0026 \\text{由式(8.15)代入得到} \\\\ \u0026= \\mathop{\\mathrm{argmax}}\\limits_{h}\\mathbb{E}_{x \\sim D_t}[f(x)h(x)] \\end{aligned} \\tag{8.16} $$$$ f(x)h(x) = 1 - 2\\mathbb{I}(f(x) \\neq h(x)) \\tag{8.17} $$$$ \\begin{aligned} h_t(x) \u0026= \\mathop{\\mathrm{argmax}}\\limits_{h}\\mathbb{E}_{x \\sim D_t}[f(x)h(x)] \u0026 \\text{式(8.16)} \\\\ \u0026= \\mathop{\\mathrm{argmax}}\\limits_{h}\\mathbb{E}_{x \\sim D_t}[1 - 2\\mathbb{I}(f(x) \\neq h(x))] \u0026 \\text{由式(8.17)代入得到} \\\\ \u0026= \\mathop{\\mathrm{argmin}}\\limits_{h}\\mathbb{E}_{x \\sim D_t}[\\mathbb{I}(f(x) \\neq h(x))] \u0026 \\text{去掉常数和负号} \\end{aligned} \\tag{8.18} $$由此可见，理想的 \\( h_t(x) \\) 将在分布 \\( D_t \\) 下最小化分类误差。\n因此，弱分类器将基于分布 \\( D_t \\) 进行训练，且针对 \\( D_t \\) 的分类误差应小于 0.5，这在一定程度上类似“残差逼近”的思想。\n$$ \\begin{aligned} D_{t+1}(x) \u0026= \\dfrac{D(x)e^{-f(x)H_t(x)}}{\\mathbb{E}_{x \\sim D}[e^{-f(x)H_t(x)}]} \u0026 \\text{由式(8.15)可得} \\\\ \u0026= \\dfrac{D(x)e^{-f(x)H_{t-1}(x)} e^{-f(x)\\alpha_th_t(x)}}{\\mathbb{E}_{x \\sim D}[e^{-f(x)H_t(x)}]} \u0026 \\text{将 } H_t(x) \\text{ 展开得到} \\\\ \u0026= D_t(x)e^{-f(x)\\alpha_th_t(x)}\\dfrac{\\mathbb{E}_{x \\sim D}[e^{-f(x)H_{t-1}(x)}]}{\\mathbb{E}_{x \\sim D}[e^{-f(x)H_t(x)}]} \u0026 \\text{将式(8.15)代入，构造 } D_t(x) \\end{aligned} \\tag{8.19} $$ 至此，我们求得了\\(AdaBoost\\)算法的样本分布更新公式。\nXGBOOST Bagging 算法介绍 策略：\n从样本集中有放回(使得下次采样时还可选中)的采样出\\(m\\)个样本，重复操作，我们可以得到\\(T\\)个数量为\\(m\\)的训练样本集，然后基于每一个采样集训练出一个基学习器，再将这些基学习器进行结合，然后进行输出的预测。如果是分类任务，通常使用 简单投票法，对回归任务使用简单平均法。\n采样自助采样法的好处：初始训练集中约有63.2%的样本出现在采样集中，剩下的36.8%的样本可用作验证集。对于这部分大约36.8%的没有被采样到的数据，我们常常称之为袋外数据(\\(Out Of Bag, 简称OOB\\))。这些数据没有参与训练集模型的拟合，因此可以用来检测模型的泛化能力。\n算法流程 用较为通俗的话解释就是在每一轮训练中，通过自助采样获得样本集，使用这个样本集训练一个弱学习器，最后将所有的弱学习器进行结合。\n随机森林 随机森林(\\(Random Forest,简称RF\\))算法在\\(bagging\\)算法上进行了一点独特的改进，体现在以下几点。\n\\(1\\).\\(RF\\)以决策树为基学习器。\n\\(2\\).在训练过程中引入了随机属性选择。\n随机属性选择 传统决策树在选择划分属性是在当前结点的属性集合中选择一个最优属性。而\\(RF\\)有所不同，对基决策树的每个结点，先从该结点的属性集合(假设有\\(d\\)个属性)中随机选择一个包含\\(k\\)个属性的子集，然后再从这个子集中选择一个最优属性用于划分。 若\\(K=1\\)，相当于随机选择属性，\\(k=d\\)，则基决策树的构建与传统决策树相同，一般情况下，推荐\\(k=log2(d)\\)。\n算法 输入为样本集 \\(D={(x_1,y_1),(x_2,y_2),...(x_m,y_m)}\\)，弱分类器迭代次数\\(T\\)。输出为最终的强分类器\\(f(x)\\)　\\((1)\\)对于\\(t=1,2...,T\\):\n\\((a)\\)对训练集进行第\\(t\\)次随机采样，共采集\\(m\\)次，得到包含\\(m\\)个样本的采样集\\(D_t\\)\n\\((b)\\)用采样集\\(D_t\\)训练第t个决策树模型\\(G_t (x)\\)，在训练决策树模型的节点的时候， 在节点上所有的样本特征中选择一部分样本特征， 在这些随机选择的部分样本特征中选择一个最优的特征来做决策树的左右子树划分\n\\((2)\\)如果是分类算法预测，则\\(T\\)个弱学习器投出最多票数的类别或者类别之一为最终类别。如果是回归算法，\\(T\\)个弱学习器得到的回归结果进行算术平均得到的值为最终的模型输出。\n随机森林的优点 1.并行处理：由于每棵树的构建是相互独立的，随机森林可以很容易地实现并行化，提高训练效率。\n2.防止过拟合：随机森林通过引入随机性，降低了模型对训练数据的依赖，从而减少了过拟合的风险。\n3.高准确性：通过构建多个决策树并将结果进行投票或平均，随机森林能够提高模型的整体准确性。\n随机森林的缺点 1.在某些噪音比较大的特征上，\\(RF\\) 模型容易陷入过拟合；\n2.取值比较多的划分特征对\\(RF\\)的决策会产生更大的影响，从而有可能影响模型的效果。\n","date":"2024-10-20T15:19:41+08:00","permalink":"https://zhangherbal.github.io/p/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/","title":"机器学习-集成学习"},{"content":"支持向量机 前言 本篇文章对B站一些高质量视频，看过的高质量文章，西瓜书等著作进行白话总结\n支持向量机-线性模型 知识补充 补充1:超平面的定义 超平面是指\\(n\\)维线性空间中维度为\\(n-1\\)的子空间。它可以把线性空间分割成不相交的两部分。比如二维空间中，一条直线是一维的，它把平面分成了两块；三维空间中，一个平面是二维的，它把空间分成了两块。\n补充2：拉格朗日乘子法 在本章只给出基本介绍，具体推导及原理将单开一章进行阐述。 拉格朗日乘子法是用于求解有约束的优化问题的数学方法，本章涉及的流程为\n\\(1\\).构造拉格朗日函数\n\\(2\\).解变量的偏导方程\n\\(3\\).代入目标函数即可\n\\(4\\).\\(KKT\\)条件\n若约束为不等式约束就要满足\\(KKT\\)条件：在约束\\(g(x)\u003c=0\\)下最小化\\(f(x)\\),可转化为在如下约束下最小化拉格朗日函数： 设 \\( a \\) 为拉格朗日乘子，\\(KKT\\)条件为：\n\\[ g(x) \\leq 0, \\quad a \\geq 0, \\quad a \\cdot g(x) = 0 \\]问题引出 支持向量机可以用作分类算法\n分类算法的思想：给定训练样本集 \\( D = \\{ (x_1, y_1),(x_2,y_2),...(x_m,y_m) \\} \\) ,\\(y_i∈\\{-1,+1\\},\\) ，基于训练集\\(D\\)在样本空间中找到一个划分超平面，将不同类别的样本分开，但是有很多超平面都能将训练样本分开，因此我们要找到一个最优的超平面完成分类任务。\n训练样本集分为线性可分训练集和非线性可分训练集，线性可分样本集意味着我们可以用一个超平面把样本集分成两个部分\n图中共有五个超平面对我们的样本集进行划分，在这五个超平面中哪一个是最优秀的超平面呢?通过观察可以发现，超平面\\(3\\)对误差容忍性最好，为什么呢？为了验证误差的容忍性，我们首先假设存在误差，数据集中的向量的位置可能发生错误，观察图中的箭头，平面\\(1\\)和平面\\(5\\)无法容忍向量发生误差导致变化(因为导致了分类错误)，通过观察可以发现其余平面(特别偏向某一侧数据点)对误差的容忍性相对较弱，非常容易出现分类错误。而平面\\(3\\)受到的影响最小，因此我们选择了超平面\\(3\\)，那么我们该如何去描述我们刚刚决策的过程呢？\n我们要定义一个性能指标来帮助我们去寻找这个超平面。在下图中我们将\\(1\\)号超平面向两侧进行平移，直到擦到两侧数据集中的圆圈和叉叉停下，这也说明了被擦中的向量离\\(1\\)号超平面最近，我们称这些向量为支持向量\\((support vector)\\)。停下后此时又生成了平面\\(2\\)和平面\\(3\\)，我们将平面\\(2\\)和平面\\(3\\)之间的距离\\(d\\)称为间隔\\((margin)\\)，官方一点我们也可以称之为两个异类支持向量到超平面的距离，我们将间隔作为性能指标，我们要做的事情就是将\\(d\\)最大化，除此之外，我们会发现一个问题，跟这个超平面平行的超平面其实都能做到让\\(margin\\)最大化，因此它们都能实现到平面\\(2\\)和平面\\(3\\)的距离之和为\\(d\\),因此我们设置限定条件，即这个超平面到平面\\(2\\)和平面\\(3\\)的距离都是\\(\\frac{d}{2}\\)。 接下来我们就进入数学世界来进行推导。\n数学描述 定义 定义1：训练数据及标签 \\( D = \\{ (x_1, y_1),(x_2,y_2),...(x_m,y_m) \\} \\) ,\\(y_i∈\\{-1,+1\\},\\)，\\(x_i是多维向量,y_i是标签\\)，\\(y_i\\)通常取\\(+1\\)或者\\(-1\\)。\n定义2： 超平面方程 超平面方程为\\(w^Tx+b=0\\),其中\\(w\\)也是向量，与\\(x\\)的维度相同。\n定义3： 线性模型的任务 即确定超平面方程中的\\(w\\)和\\(b\\)，确定最终的超平面方程。\n定义4：训练集的线性可分 对于\\( (x_i, y_i), \\, i \\in \\{1, 2, \\ldots, N\\} \\)，\\(\\exists(w,b)\\)，使\\(\\forall i \\in \\{1, 2, \\ldots, N\\}\\)有\n\\(a\\).若\\(y_i=+1\\)，则\\(w^Tx+b≥0\\)\n\\(b\\).若\\(y_i=-1\\)，则\\(w^Tx+b\u003c0\\)\n将上述两式进行结合可得 \\(y_i(w^Tx+b) \\geq 0\\) \\((1)\\)\n推导过程 条件1： \\(w^Tx+b=0\\) 与 \\(aw^Tx+ab=0\\)是同一个平面，\\(a\\)是正整数，如果前者满足公式\\((1)\\)，后者也满足。\n条件2： 平面 \\(w_1x+w_2y+b=0\\)\n则\\((x_0,y_0)\\)到此平面距离d的值为 \\( d= \\frac{|w_1 x_0 + w_2 y_0 + b|}{\\sqrt{w_1^2 + w_2^2}}\\)\n向量\\(x_0\\)到超平面\\(w^Tx+b=0\\)的距离为\\( \\frac{|w^T x_0 + b|}{\\|w\\|}\\)\n推导开始 假设支持向量为\\(x_0\\)，我们的任务是最大化间隔\\((margin)\\)，也就是最大化\\(d\\)，\\(d\\)由支持向量到超平面距离公式可得，但是观察公式我们貌似无法解出值，此时我们让\\(w^T x_0+b=1\\)，那么支持向量所在超平面方程变为\\(w^T x_0+b=1\\)和\\(w^T x_0+b=-1\\)且距离直接就变成了\\( \\frac{2}{\\|w\\|}\\)，但是凭啥呢？ 假设\\(d_0=|w^T x_0+b|\\)，根据条件1，同时缩放\\(w,b\\)后平面不变，因此得到\\(d_0=|a(w^T x_0+b)|\\)，a可以通过取值使\\(w^T x_0+b=1\\)。因为要最大化间隔，因此只要最小化分母即可。\n除此之外，我们发现终极目标中的限制条件也发生了变化，支持向量所在平面都是\\(w^T x_0+b=1\\)，那么其他的向量确定的平面肯定是大于等于\\(1\\)，乘以\\(y_i\\)是为了保证\\((1)\\)的成立。\n终极目标 式\\((2)\\)\n最小化 \\((minimise)\\)\n\\( \\|w\\|^2 \\)\n限制条件\\((subject\\ to)\\)\n\\(y_i (w^T x_i + b) \\geq 1, \\quad \\forall i \\in \\{1, 2, \\ldots, N\\}\\) 优化方法 拉格朗日乘子法 $$ \\begin{align*} \\text{maximize} \u0026 \\quad \\sum_{i=1}^{m} \\alpha_i - \\frac{1}{2} \\sum_{i=1}^{m} \\sum_{j=1}^{m} \\alpha_i \\alpha_j y_i y_j (x_i^T \\cdot x_j) \\\\ \\text{subject to} \u0026 \\quad \\sum_{i=1}^{m} \\alpha_i y_i = 0, \\\\ \u0026 \\quad \\alpha_i \\geq 0 \\quad \\forall i\\in \\{1, 2, \\ldots, m\\}. \\end{align*} $$ 因为是不等式约束于是还要满足\\(KKT\\)条件:\n\\(α\u003e=0,y_if(x_i)-1\u003e=0,α_i(y_if(x_i)-1)=0\\)\n通过观察\\(α_i(y_if(x_i)-1)=0\\)得出结论要么\\(α_i=0\\)，要么\\(y_if(x_i)=1\\)，又因为\\(|y_i|=1\\)得出\\(|f(x)|=1\\),也就是满足\\(α_i\u003e0\\)的点即为支持向量，我们在式\\((4)\\)中已经得出了\\(w\\),若知道支持向量\\((x_k,y_k)\\)代回即可得出 \\(b=y_k-\\sum_{i=1}^{m}α_ix_i^Ty_ix_k\\)\n支持向量机-非线性模型 模型修正 如果训练集样本是非线性可分样本集，那么我们按照线性模型进行操作就无法找出对应的\\(w,b\\)。因此我们要对模型进行改进。\n修正1 为了避免过拟合以及保证更优越的鲁棒性，我们引入松弛变量和正则项进行调和。在很多场合也被称为软间隔\\(SVM\\)。\n最小化 \\((minimise)(7)\\)\n\\(\\frac{1}{2} \\|w\\|^2 + C \\sum_{i=1}^N \\xi_i\\)\n限制条件\\((subject\\ to)(8)\\)\n\\(\\quad y_i (w^T x_i + b) \\geq 1 - \\xi_i, \\quad \\forall i \\in \\{1, 2, \\ldots, N\\}\\)\n\\(\\xi_i \\geq 0, \\quad \\forall i \\in \\{1, 2, \\ldots, N\\}\\)\n通过观察我们可以发现，如果\\(\\xi_i\\)足够大，那么式\\((8)\\)一定会得到满足，而我们在式\\((7)\\)中最小化目标函数又保证了\\(\\xi_i\\)不会特别大。在式\\((7)\\)中\\(C\\)是预设好的未知常数，需要提前设置，具体值要根据实践确定。我们通常称\\(\\xi_i\\)为松弛变量， \\(C \\sum_{i=1}^N \\xi_i\\)为正则项。\n修正2 我们做到第一步修正其实还不够，如下图，真正能成功分类样本的其实是一个椭圆，但是我们的模型决定了它是一条直线或者是超平面。因此我们需要将低维的x向高维映射，这样线性可分的概率就变大。\n高维映射\n\\(x-\u003e\\phi(x)\\)\n最小化 \\((minimise)(7)\\)\n\\(\\frac{1}{2} \\|w\\|^2 + C \\sum_{i=1}^N \\xi_i\\)\n限制条件\\((subject\\ to)(8)\\)\n\\(\\quad y_i (w^T \\phi(x_i) + b) \\geq 1 - \\xi_i, \\quad \\forall i \\in \\{1, 2, \\ldots, N\\}\\)\n\\(\\xi_i \\geq 0, \\quad \\forall i \\in \\{1, 2, \\ldots, N\\}\\)\n现在的\\(w\\)维度也将发生变化。\n核函数 那么我们该如何去选取这个\\(\\phi(x)\\)呢，根据\\(SVM\\)的原理，我们假设\\(\\phi(x)\\)是无限维，那么我们发现我们的问题将会变得不可做，那么该如何解决这个问题呢。 SVM精妙之处将被引出： 我们可以不知道\\(\\phi(x)\\)的显式表达，但是我们可以通过核函数(\\(Kernal\\ function\\))把这个问题变得可做。\n\\(k(x1,x2)=\\phi(x_1)^T \\phi(x_2)\\)(后面两者相乘明显是一个数值)\n\\[ K(x, y) = x^T y=\\phi(x_1)^T \\phi(x_2) \\] \\[ K(x, y) = (x^T y + c)^d=\\phi(x_1)^T \\phi(x_2) \\] \\[ K(x, y) = \\exp\\left(-\\frac{\\|x - y\\|^2}{2\\sigma^2}\\right)=\\phi(x_1)^T \\phi(x_2) \\] \\[ K(x, y) = \\tanh(\\alpha x^T y + c)=\\phi(x_1)^T \\phi(x_2) \\]\n充要条件 \\(k(x_1,x_2)=\\phi(x_1)^T \\phi(x_2)\\)的充要条件是：(推导暂无)\n交换性：\\(k(x_1,x_2)=k(x_2,x_1)\\)\n半正定性:\\(\\forall c_i\u003e0,x_i\u003e0 (\\sum_{i=1}^N \\sum_{j=1}^N c_ic_jk(x_1,x_2)\u003e=0) \\) \\(\\quad \\forall i \\in \\{1, 2, \\ldots, N\\}\\)\n我们最常用的就是高斯核，我们可以通过继续转化问题来使用核函数去解决这个问题，把这个优化问题变得可做。\n数学推导 拉格朗日乘子法 \\[ \\mathcal{L}(\\mathbf{w}, b, \\boldsymbol{\\alpha}, \\boldsymbol{\\xi},u) = \\frac{1}{2} \\|\\mathbf{w}\\|^2 + C \\sum_{i=1}^{m} \\xi_i - \\sum_{i=1}^{m} \\alpha_i \\left( y_i (\\mathbf{w}^T \\phi(x_i) + b) - 1 + \\xi_i \\right) - \\]\\[ \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{w}} = \\mathbf{w} - \\sum_{i=1}^{m} \\alpha_i y_i \\phi(x_i) = 0 \\]\\[ \\mathbf{w} = \\sum_{i=1}^{m} \\alpha_i y_i \\phi(x_i) \\]\\[ \\frac{\\partial \\mathcal{L}}{\\partial b} = -\\sum_{i=1}^{m} \\alpha_i y_i = 0 \\]\\[ \\frac{\\partial \\mathcal{L}}{\\partial \\xi_i} = C - \\alpha_i - \\mu_i = 0 \\]\\[ \\mathcal{L}(\\boldsymbol{\\alpha}) = \\sum_{i=1}^{m} \\alpha_i - \\frac{1}{2} \\sum_{i=1}^{m} \\sum_{j=1}^{m} \\alpha_i \\alpha_j y_i y_j \\phi(\\mathbf{x}_i)^T \\phi(\\mathbf{x}_j) \\]\\[ \\mathcal{L}(\\boldsymbol{\\alpha}) = \\sum_{i=1}^{m} \\alpha_i - \\frac{1}{2} \\sum_{i=1}^{m} \\sum_{j=1}^{m} \\alpha_i \\alpha_j y_i y_j k(x_i,x_j) \\]\\[ \\mathcal{L}(\\boldsymbol{\\alpha}) = \\sum_{i=1}^{m} \\alpha_i - \\frac{1}{2} \\sum_{i=1}^{m} \\sum_{j=1}^{m} \\alpha_i \\alpha_j y_i y_j k(x_i,x_j) \\] 限制条件: \\(0 \\leq a_i \\leq c\\)\\(\\quad \\forall i \\in \\{1, 2, \\ldots, m\\}\\)\n\\( \\sum_{i=1}^{m} \\alpha_i y_i = 0\\) 现在我们\\(y_i,y_j,k(x_i,x_j)\\)都能求出，只剩\\(α\\)的值，这还是一个凸优化问题，我们要SMO算法进行解决，还有其他的一些算法，可以选择一种去学习。 以上就是SVM非线性模型的理论内容。\n测试过程 我们发现我们无法直接求出\\(w\\),但我们只要知道分类完毕的结果就好\n给出测试样本\\(X\\) 若\\(w^T(\\phi(x))+b \\geq 0\\),则 \\(y=1\\)。 若\\(w^T(\\phi(x))+b \u003c 0\\),则 \\(y=-1\\)。\n\\(w^T\\phi(x)=\\sum_{i=1}^{N}α_iy_i \\phi(x_i)^T\\phi(x)\\)\n\\(w^T\\phi(x)=\\sum_{i=1}^{N}α_iy_i k(x_1,x)\\)\n根据\\(KKT\\)条件求出\\(b\\)的值即可，类似线性模型推导\n","date":"2024-10-06T14:27:00+08:00","permalink":"https://zhangherbal.github.io/p/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/","title":"机器学习-支持向量机"},{"content":"正文测试 而这些并不是完全重要，更加重要的问题是， 带着这些问题，我们来审视一下学生会退会。 既然如何， 对我个人而言，学生会退会不仅仅是一个重大的事件，还可能会改变我的人生。 我们不得不面对一个非常尴尬的事实，那就是， 可是，即使是这样，学生会退会的出现仍然代表了一定的意义。 学生会退会，发生了会如何，不发生又会如何。 经过上述讨论， 生活中，若学生会退会出现了，我们就不得不考虑它出现了的事实。 学生会退会，到底应该如何实现。 这样看来， 在这种困难的抉择下，本人思来想去，寝食难安。 对我个人而言，学生会退会不仅仅是一个重大的事件，还可能会改变我的人生。 就我个人来说，学生会退会对我的意义，不能不说非常重大。 莎士比亚曾经提到过，人的一生是短的，但如果卑劣地过这一生，就太长了。这似乎解答了我的疑惑。 莫扎特说过一句富有哲理的话，谁和我一样用功，谁就会和我一样成功。这启发了我， 对我个人而言，学生会退会不仅仅是一个重大的事件，还可能会改变我的人生。 学生会退会，到底应该如何实现。 一般来说， 从这个角度来看， 这种事实对本人来说意义重大，相信对这个世界也是有一定意义的。 在这种困难的抉择下，本人思来想去，寝食难安。 了解清楚学生会退会到底是一种怎么样的存在，是解决一切问题的关键。 一般来说， 生活中，若学生会退会出现了，我们就不得不考虑它出现了的事实。 问题的关键究竟为何？ 而这些并不是完全重要，更加重要的问题是。\n奥斯特洛夫斯基曾经说过，共同的事业，共同的斗争，可以使人们产生忍受一切的力量。　带着这句话，我们还要更加慎重的审视这个问题： 一般来讲，我们都必须务必慎重的考虑考虑。 既然如此， 这种事实对本人来说意义重大，相信对这个世界也是有一定意义的。 带着这些问题，我们来审视一下学生会退会。 我认为， 我认为， 在这种困难的抉择下，本人思来想去，寝食难安。 问题的关键究竟为何？ 每个人都不得不面对这些问题。 在面对这种问题时， 要想清楚，学生会退会，到底是一种怎么样的存在。 我认为， 既然如此， 每个人都不得不面对这些问题。 在面对这种问题时， 那么， 我认为， 学生会退会因何而发生。\n引用 思念是最暖的忧伤像一双翅膀\n让我停不了飞不远在过往游荡\n不告而别的你 就算为了我着想\n这么沉痛的呵护 我怎么能翱翔\n最暖的憂傷 - 田馥甄\n图片 1 2 3 ![Photo by Florian Klauer on Unsplash](florian-klauer-nptLmg6jqDo-unsplash.jpg) ![Photo by Luca Bravo on Unsplash](luca-bravo-alS7ewQ41M8-unsplash.jpg) ![Photo by Helena Hertz on Unsplash](helena-hertz-wWZzXlDpMog-unsplash.jpg) ![Photo by Hudai Gayiran on Unsplash](hudai-gayiran-3Od_VKcDEAA-unsplash.jpg) 相册语法来自 Typlog\n","date":"2020-09-09T00:00:00Z","image":"https://zhangherbal.github.io/p/test-chinese/helena-hertz-wWZzXlDpMog-unsplash_hu_2307260c751d0e0b.jpg","permalink":"https://zhangherbal.github.io/p/test-chinese/","title":"Chinese Test"},{"content":"This article offers a sample of basic Markdown syntax that can be used in Hugo content files, also it shows whether basic HTML elements are decorated with CSS in a Hugo theme.\nHeadings The following HTML \u0026lt;h1\u0026gt;—\u0026lt;h6\u0026gt; elements represent six levels of section headings. \u0026lt;h1\u0026gt; is the highest section level while \u0026lt;h6\u0026gt; is the lowest.\nH1 H2 H3 H4 H5 H6 Paragraph Xerum, quo qui aut unt expliquam qui dolut labo. Aque venitatiusda cum, voluptionse latur sitiae dolessi aut parist aut dollo enim qui voluptate ma dolestendit peritin re plis aut quas inctum laceat est volestemque commosa as cus endigna tectur, offic to cor sequas etum rerum idem sintibus eiur? Quianimin porecus evelectur, cum que nis nust voloribus ratem aut omnimi, sitatur? Quiatem. Nam, omnis sum am facea corem alique molestrunt et eos evelece arcillit ut aut eos eos nus, sin conecerem erum fuga. Ri oditatquam, ad quibus unda veliamenimin cusam et facea ipsamus es exerum sitate dolores editium rerore eost, temped molorro ratiae volorro te reribus dolorer sperchicium faceata tiustia prat.\nItatur? Quiatae cullecum rem ent aut odis in re eossequodi nonsequ idebis ne sapicia is sinveli squiatum, core et que aut hariosam ex eat.\nBlockquotes The blockquote element represents content that is quoted from another source, optionally with a citation which must be within a footer or cite element, and optionally with in-line changes such as annotations and abbreviations.\nBlockquote without attribution Tiam, ad mint andaepu dandae nostion secatur sequo quae. Note that you can use Markdown syntax within a blockquote.\nBlockquote with attribution Don\u0026rsquo;t communicate by sharing memory, share memory by communicating.\n— Rob Pike1\nTables Tables aren\u0026rsquo;t part of the core Markdown spec, but Hugo supports supports them out-of-the-box.\nName Age Bob 27 Alice 23 Inline Markdown within tables Italics Bold Code italics bold code A B C D E F Lorem ipsum dolor sit amet, consectetur adipiscing elit. Phasellus ultricies, sapien non euismod aliquam, dui ligula tincidunt odio, at accumsan nulla sapien eget ex. Proin eleifend dictum ipsum, non euismod ipsum pulvinar et. Vivamus sollicitudin, quam in pulvinar aliquam, metus elit pretium purus Proin sit amet velit nec enim imperdiet vehicula. Ut bibendum vestibulum quam, eu egestas turpis gravida nec Sed scelerisque nec turpis vel viverra. Vivamus vitae pretium sapien Code Blocks Code block with backticks 1 2 3 4 5 6 7 8 9 10 \u0026lt;!doctype html\u0026gt; \u0026lt;html lang=\u0026#34;en\u0026#34;\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026#34;utf-8\u0026#34;\u0026gt; \u0026lt;title\u0026gt;Example HTML5 Document\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;p\u0026gt;Test\u0026lt;/p\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; Code block indented with four spaces \u0026lt;!doctype html\u0026gt; \u0026lt;html lang=\u0026quot;en\u0026quot;\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026quot;utf-8\u0026quot;\u0026gt; \u0026lt;title\u0026gt;Example HTML5 Document\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;p\u0026gt;Test\u0026lt;/p\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; Code block with Hugo\u0026rsquo;s internal highlight shortcode 1 2 3 4 5 6 7 8 9 10 \u0026lt;!doctype html\u0026gt; \u0026lt;html lang=\u0026#34;en\u0026#34;\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026#34;utf-8\u0026#34;\u0026gt; \u0026lt;title\u0026gt;Example HTML5 Document\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;p\u0026gt;Test\u0026lt;/p\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; Diff code block 1 2 3 4 5 [dependencies.bevy] git = \u0026#34;https://github.com/bevyengine/bevy\u0026#34; rev = \u0026#34;11f52b8c72fc3a568e8bb4a4cd1f3eb025ac2e13\u0026#34; - features = [\u0026#34;dynamic\u0026#34;] + features = [\u0026#34;jpeg\u0026#34;, \u0026#34;dynamic\u0026#34;] List Types Ordered List First item Second item Third item Unordered List List item Another item And another item Nested list Fruit Apple Orange Banana Dairy Milk Cheese Other Elements — abbr, sub, sup, kbd, mark GIF is a bitmap image format.\nH2O\nXn + Yn = Zn\nPress CTRL + ALT + Delete to end the session.\nMost salamanders are nocturnal, and hunt for insects, worms, and other small creatures.\nHyperlinked image The above quote is excerpted from Rob Pike\u0026rsquo;s talk during Gopherfest, November 18, 2015.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","date":"2019-03-11T00:00:00Z","image":"https://zhangherbal.github.io/p/markdown-syntax-guide/pawel-czerwinski-8uZPynIu-rQ-unsplash_hu_e95a4276bf860a84.jpg","permalink":"https://zhangherbal.github.io/p/markdown-syntax-guide/","title":"Markdown Syntax Guide"},{"content":"Lorem est tota propiore conpellat pectoribus de pectora summo.\nRedit teque digerit hominumque toris verebor lumina non cervice subde tollit usus habet Arctonque, furores quas nec ferunt. Quoque montibus nunc caluere tempus inhospita parcite confusaque translucet patri vestro qui optatis lumine cognoscere flos nubis! Fronde ipsamque patulos Dryopen deorum.\nExierant elisi ambit vivere dedere Duce pollice Eris modo Spargitque ferrea quos palude Rursus nulli murmur; hastile inridet ut ab gravi sententia! Nomine potitus silentia flumen, sustinet placuit petis in dilapsa erat sunt. Atria tractus malis.\nComas hunc haec pietate fetum procerum dixit Post torum vates letum Tiresia Flumen querellas Arcanaque montibus omnes Quidem et Vagus elidunt The Van de Graaf Canon\nMane refeci capiebant unda mulcebat Victa caducifer, malo vulnere contra dicere aurato, ludit regale, voca! Retorsit colit est profanae esse virescere furit nec; iaculi matertera et visa est, viribus. Divesque creatis, tecta novat collumque vulnus est, parvas. Faces illo pepulere tempus adest. Tendit flamma, ab opes virum sustinet, sidus sequendo urbis.\nIubar proles corpore raptos vero auctor imperium; sed et huic: manus caeli Lelegas tu lux. Verbis obstitit intus oblectamina fixis linguisque ausus sperare Echionides cornuaque tenent clausit possit. Omnia putatur. Praeteritae refert ausus; ferebant e primus lora nutat, vici quae mea ipse. Et iter nil spectatae vulnus haerentia iuste et exercebat, sui et.\nEurytus Hector, materna ipsumque ut Politen, nec, nate, ignari, vernum cohaesit sequitur. Vel mitis temploque vocatus, inque alis, oculos nomen non silvis corpore coniunx ne displicet illa. Crescunt non unus, vidit visa quantum inmiti flumina mortis facto sic: undique a alios vincula sunt iactata abdita! Suspenderat ego fuit tendit: luna, ante urbem Propoetides parte.\n","date":"2019-03-09T00:00:00Z","image":"https://zhangherbal.github.io/p/placeholder-text/matt-le-SJSpo9hQf7s-unsplash_hu_c1ca39d792aee4ab.jpg","permalink":"https://zhangherbal.github.io/p/placeholder-text/","title":"Placeholder Text"},{"content":"Mathematical notation in a Hugo project can be enabled by using third party JavaScript libraries.\nIn this example we will be using KaTeX\nCreate a partial under /layouts/partials/math.html Within this partial reference the Auto-render Extension or host these scripts locally. Include the partial in your templates like so: 1 2 3 {{ if or .Params.math .Site.Params.math }} {{ partial \u0026#34;math.html\u0026#34; . }} {{ end }} To enable KaTeX globally set the parameter math to true in a project\u0026rsquo;s configuration To enable KaTeX on a per page basis include the parameter math: true in content files Note: Use the online reference of Supported TeX Functions\nExamples Inline math: $\\varphi = \\dfrac{1+\\sqrt5}{2}= 1.6180339887…$\n$$ \\varphi = 1+\\frac{1} {1+\\frac{1} {1+\\frac{1} {1+\\cdots} } } $$","date":"2019-03-08T00:00:00Z","permalink":"https://zhangherbal.github.io/p/math-typesetting/","title":"Math Typesetting"},{"content":"Emoji can be enabled in a Hugo project in a number of ways.\nThe emojify function can be called directly in templates or Inline Shortcodes.\nTo enable emoji globally, set enableEmoji to true in your site\u0026rsquo;s configuration and then you can type emoji shorthand codes directly in content files; e.g.\n🙈 :see_no_evil: 🙉 :hear_no_evil: 🙊 :speak_no_evil:\nThe Emoji cheat sheet is a useful reference for emoji shorthand codes.\nN.B. The above steps enable Unicode Standard emoji characters and sequences in Hugo, however the rendering of these glyphs depends on the browser and the platform. To style the emoji you can either use a third party emoji font or a font stack; e.g.\n1 2 3 .emoji { font-family: Apple Color Emoji, Segoe UI Emoji, NotoColorEmoji, Segoe UI Symbol, Android Emoji, EmojiSymbols; } ","date":"2019-03-05T00:00:00Z","image":"https://zhangherbal.github.io/p/emoji-support/the-creative-exchange-d2zvqp3fpro-unsplash_hu_27b8954607cdb515.jpg","permalink":"https://zhangherbal.github.io/p/emoji-support/","title":"Emoji Support"}]